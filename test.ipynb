{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 18:31:57.370797: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-10 18:31:58.672934: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-10 18:31:58.673062: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-10 18:31:58.673079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING: AstropyDeprecationWarning: \"verbose\" was deprecated in version 1.15.0 and will be removed in a future version.  [useful_functions]\n",
      "WARNING: AstropyDeprecationWarning: \"verbose\" was deprecated in version 1.15.0 and will be removed in a future version.  [useful_functions]\n",
      "WARNING: AstropyDeprecationWarning: \"verbose\" was deprecated in version 1.15.0 and will be removed in a future version.  [useful_functions]\n",
      "2023-06-10 18:32:03.450491: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-10 18:32:03.450737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (coka.fe.infn.it): /proc/driver/nvidia/version does not exist\n",
      "2023-06-10 18:32:03.451627: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 - 5s - loss: 0.1180 - sigma_loss: 0.0687 - sigma_batch_loss: 0.1180 - mse_tau: 0.0667 - mse_sigma: 0.0019 - sigma_f_loss: 194371.5312 - mse_batch: 0.0513 - val_loss: 0.0229 - val_sigma_loss: 0.0282 - val_sigma_batch_loss: 0.0229 - val_mse_tau: 0.0189 - val_mse_sigma: 0.0093 - val_sigma_f_loss: 925679.6875 - val_mse_batch: 0.0040 - 5s/epoch - 962ms/step\n",
      "Epoch 2/5\n",
      "5/5 - 0s - loss: 0.0173 - sigma_loss: 0.0350 - sigma_batch_loss: 0.0173 - mse_tau: 0.0135 - mse_sigma: 0.0215 - sigma_f_loss: 2151180.5000 - mse_batch: 0.0038 - val_loss: 0.0169 - val_sigma_loss: 0.0261 - val_sigma_batch_loss: 0.0169 - val_mse_tau: 0.0014 - val_mse_sigma: 0.0247 - val_sigma_f_loss: 2466807.2500 - val_mse_batch: 0.0155 - 237ms/epoch - 47ms/step\n",
      "Epoch 3/5\n",
      "5/5 - 0s - loss: 0.0200 - sigma_loss: 0.0412 - sigma_batch_loss: 0.0200 - mse_tau: 0.0084 - mse_sigma: 0.0329 - sigma_f_loss: 3286584.5000 - mse_batch: 0.0116 - val_loss: 0.0115 - val_sigma_loss: 0.0232 - val_sigma_batch_loss: 0.0115 - val_mse_tau: 0.0044 - val_mse_sigma: 0.0189 - val_sigma_f_loss: 1888632.6250 - val_mse_batch: 0.0072 - 229ms/epoch - 46ms/step\n",
      "Epoch 4/5\n",
      "5/5 - 0s - loss: 0.0157 - sigma_loss: 0.0314 - sigma_batch_loss: 0.0157 - mse_tau: 0.0101 - mse_sigma: 0.0212 - sigma_f_loss: 2123673.2500 - mse_batch: 0.0055 - val_loss: 0.0047 - val_sigma_loss: 0.0097 - val_sigma_batch_loss: 0.0047 - val_mse_tau: 0.0023 - val_mse_sigma: 0.0074 - val_sigma_f_loss: 740599.8125 - val_mse_batch: 0.0024 - 225ms/epoch - 45ms/step\n",
      "Epoch 5/5\n",
      "5/5 - 0s - loss: 0.0064 - sigma_loss: 0.0118 - sigma_batch_loss: 0.0064 - mse_tau: 0.0041 - mse_sigma: 0.0076 - sigma_f_loss: 762295.3750 - mse_batch: 0.0023 - val_loss: 0.0028 - val_sigma_loss: 0.0043 - val_sigma_batch_loss: 0.0028 - val_mse_tau: 0.0022 - val_mse_sigma: 0.0021 - val_sigma_f_loss: 213912.5781 - val_mse_batch: 6.0723e-04 - 215ms/epoch - 43ms/step\n",
      "Saving model to disk\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "#i import the necessary libraries\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import nnhealpix.layers\n",
    "from tensorflow.keras import metrics\n",
    "import pandas as pd\n",
    "from loss_functions import sigma_loss, sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma, mse_batch, sigma_f_loss\n",
    "import math\n",
    "import useful_functions as uf\n",
    "import NN_functions as nuf\n",
    "import os, shutil\n",
    "\n",
    "seed_train=33\n",
    "np.random.seed(seed_train)# i set a random seed for the generation of the maps for reproducibility\n",
    "\n",
    "#map gen\n",
    "nside = 16\n",
    "n_train=100 #the total number of training+validation pair of maps that i will generate\n",
    "n_train_fix=100 #the total number of of training maps i will spread on all the r interval -> for each r value i generate n_train_fix/len(r) maps \n",
    "kind_of_map=\"EE\"\n",
    "n_channels=2\n",
    "pol=2\n",
    "res=hp.nside2resol(nside, arcmin=False) \n",
    "sensitivity=4\n",
    "\n",
    "name='2_6_23'\n",
    "base_dir='/home/amorelli/QU_foreground_tau/'+name+'/'\n",
    "# callbacks\n",
    "reduce_lr_on_plateau = True\n",
    "p_stopping=20\n",
    "p_reduce=5\n",
    "f_reduce=0.5\n",
    "stopping_monitor=\"val_mse_batch\"\n",
    "reduce_monitor=\"val_loss\"\n",
    "metrics=[sigma_loss, sigma_batch_loss,mse_tau,mse_sigma, sigma_f_loss, mse_batch]# these are the different loss functions i have used. I use them as metrics\n",
    "\n",
    "#network structure\n",
    "one_layer=True # this is to switch between one dense layer or two dense layer\n",
    "drop=0.2\n",
    "n_layer_0=48\n",
    "n_layer_1=64\n",
    "n_layer_2=16\n",
    "if kind_of_map!=\"QU\": \n",
    "    n_inputs=n_channels\n",
    "else:\n",
    "    n_inputs=pol*n_channels\n",
    "\n",
    "#train and val\n",
    "batch_size = 16\n",
    "max_epochs = 5\n",
    "lr=0.0003 \n",
    "fval=0.1 # this is the fraction of data that i use for validation, computed on n_train_fix\n",
    "training_loss=\"new_sigma_batch\"\n",
    "loss_training=sigma_batch_loss#(0.01,True) # this is the loss i use for the training\n",
    "shuffle=False\n",
    "\n",
    "f_ = np.load('/home/amorelli/cl_generator/outfile_l_47_complete.npz') \n",
    "#print(\"outfile_R:\",f_.files) #give the keiwords for the stored arrays\n",
    "labels=f_.files\n",
    "data=f_[labels[0]]\n",
    "r=f_[labels[1]]\n",
    "r, data=uf.unison_sorted_copies(r, data)\n",
    "indexes=np.linspace(0,len(r)-1,5,dtype=int)\n",
    "r=r[indexes]\n",
    "data=data[indexes]\n",
    "\n",
    "input_folder=\"/home/amorelli/foreground_noise_maps/noise_generation\"\n",
    "input_files=os.listdir(input_folder)\n",
    "for i in range(len(input_files)):\n",
    "    input_files[i]=input_folder+\"/\"+input_files[i]\n",
    "noise_maps=uf.generate_noise_maps(n_train,n_channels,nside,pol=2,sensitivity=sensitivity,input_files=input_files)\n",
    "\n",
    "noise_E,noise_B=uf.convert_to_EB(noise_maps)\n",
    "\n",
    "maps_per_cl_gen=uf.maps_per_cl(distribution=0)\n",
    "maps_per_cl=maps_per_cl_gen.compute_maps_per_cl(r,n_train,n_train_fix)\n",
    "\n",
    "mappe_B,y_r=uf.generate_maps(data, r,n_train=n_train,nside=nside, map_per_cl=maps_per_cl, \n",
    "                             noise_maps=noise_E, beam_w=2*res, kind_of_map=kind_of_map, raw=0 , n_channels=n_channels,beam_yes=1 , verbose=0)\n",
    "\n",
    "x_train,y_train,x_val,y_val = nuf.prepare_data(y_r,mappe_B,r,n_train,n_train_fix,fval,maps_per_cl\n",
    "                                               , batch_size, batch_ordering=True)\n",
    "\n",
    "#np.savez(base_dir+\"check_r_distribution\",y_train=y_train,y_val=y_val) \n",
    "\n",
    "#rand_indexes=np.random.randint(0,len(y_train)-1,10000)\n",
    "\n",
    "#np.savez(base_dir+\"check_train_maps\",y_train=y_train[rand_indexes], x_train=x_train[rand_indexes])\n",
    "\n",
    "model=nuf.build_network(n_inputs,nside,drop,n_layer_0,n_layer_1,n_layer_2,one_layer,\n",
    "                        num_output=2,use_normalization=[False,True],use_drop=False,trainable=[True,True])\n",
    "\n",
    "history=nuf.compile_and_fit(model, x_train, y_train, x_val, y_val, batch_size, max_epochs, \n",
    "                            stopping_monitor,p_stopping,reduce_monitor,f_reduce, p_reduce,base_dir, \n",
    "                            loss_training,lr,metrics,shuffle=shuffle, verbose=2,callbacks=[True,True,False,False])\n",
    "\n",
    "print('Saving model to disk')\n",
    "#model.save(base_dir+'test_model')\n",
    "#-----------------------------------------\n",
    "hyperparameters={}\n",
    "hyperparameters[\"name\"]=name\n",
    "hyperparameters[\"loss\"]=training_loss\n",
    "hyperparameters[\"noise\"]=sensitivity\n",
    "hyperparameters[\"p_stopping\"]=p_stopping\n",
    "hyperparameters[\"p_reduce\"]=p_reduce\n",
    "hyperparameters[\"f_reduce\"]=f_reduce\n",
    "hyperparameters[\"stop-reduce\"]=stopping_monitor+\"-\"+reduce_monitor\n",
    "hyperparameters[\"lr\"]=lr\n",
    "hyperparameters[\"batch_size\"]=batch_size\n",
    "hyperparameters[\"n_layers\"]=one_layer\n",
    "if one_layer:\n",
    "    hyperparameters[\"nodes_layers\"]=n_layer_0\n",
    "else:\n",
    "    hyperparameters[\"nodes_layers\"]=str(n_layer_1)+\"-\"+str(n_layer_2)\n",
    "hyperparameters[\"comments\"]=\" \"\n",
    "hyperparameters = {k:[v] for k,v in hyperparameters.items()}\n",
    "output=pd.DataFrame(hyperparameters)\n",
    "output.to_csv(base_dir+'output.txt', index=False, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camb-kernel",
   "language": "python",
   "name": "camb-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
