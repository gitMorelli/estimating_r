{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import nnhealpix.layers\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "from loss_functions import sigma_loss, sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma, mse_batch, sigma_f_loss\n",
    "import math\n",
    "import useful_functions as uf\n",
    "import NN_functions as nuf\n",
    "import os, shutil\n",
    "\n",
    "seed_train=400\n",
    "np.random.seed(seed_train)# i set a random seed for the generation of the maps for reproducibility\n",
    "tf.random.set_seed(seed_train)\n",
    "\n",
    "#map gen\n",
    "nside = 16\n",
    "n_train=100 #the total number of training+validation pair of maps that i will generate\n",
    "n_train_fix=100 #the total number of of training maps i will spread on all the r interval -> for each r value i generate n_train_fix/len(r) maps \n",
    "kind_of_map=\"BB\"\n",
    "n_channels=2\n",
    "pol=1\n",
    "res=hp.nside2resol(nside, arcmin=False) \n",
    "sensitivity=4\n",
    "\n",
    "base_dir='/home/amorelli/pipeline/test_double/sigma/'\n",
    "base_dir_tau='/home/amorelli/pipeline/test_double/'\n",
    "test_model_folder=\"test_model\"\n",
    "# callbacks\n",
    "reduce_lr_on_plateau = True\n",
    "p_stopping=[20,20]\n",
    "p_reduce=[5,5]\n",
    "f_reduce=[0.5,0.5]\n",
    "stopping_monitor=\"val_loss\"\n",
    "reduce_monitor=\"val_loss\"\n",
    "metrics=[]\n",
    "\n",
    "#network structure\n",
    "one_layer=True # this is to switch between one dense layer or two dense layer\n",
    "drop=0.2\n",
    "n_layer_0=48\n",
    "n_layer_1=64\n",
    "n_layer_2=16\n",
    "if kind_of_map!=\"QU\": \n",
    "    n_inputs=n_channels\n",
    "else:\n",
    "    n_inputs=pol*n_channels\n",
    "\n",
    "#train and val\n",
    "batch_size = 16\n",
    "max_epochs = [2,2]\n",
    "lr=[0.001, 0.0003]\n",
    "fval=0.1 # this is the fraction of data that i use for validation, computed on n_train_fix\n",
    "loss_training=tf.keras.losses.MeanSquaredError() # this is the loss i use for the training\n",
    "shuffle=True\n",
    "norm=True\n",
    "map_norm=False\n",
    "batch_ordering=False\n",
    "distr=0\n",
    "\n",
    "f_ = np.load('/home/amorelli/cl_generator/outfile_R_000_001_seed=67.npz') \n",
    "#print(\"outfile_R:\",f_.files) #give the keiwords for the stored arrays\n",
    "labels=f_.files\n",
    "data=f_[labels[0]]\n",
    "r=f_[labels[1]]\n",
    "r, data=uf.unison_sorted_copies(r, data)\n",
    "indexes=np.linspace(0,len(r)-1,10,dtype=int)\n",
    "r=r[indexes]\n",
    "data=data[indexes]\n",
    "\n",
    "#input_folder=\"/home/amorelli/foreground_noise_maps/noise_generation\"\n",
    "#input_files=os.listdir(input_folder)\n",
    "#for i in range(len(input_files)):\n",
    "   # input_files[i]=input_folder+\"/\"+input_files[i]\n",
    "noise_maps=uf.generate_noise_maps(n_train,n_channels,nside,pol=1,sensitivity=sensitivity,input_files=None)\n",
    "\n",
    "#noise_E,noise_B=uf.convert_to_EB(noise_maps)\n",
    "\n",
    "maps_per_cl_gen=uf.maps_per_cl(distribution=distr)\n",
    "maps_per_cl=maps_per_cl_gen.compute_maps_per_cl(r,n_train,n_train_fix)\n",
    "\n",
    "mappe_B,y_r=uf.generate_maps(data, r,n_train=n_train,nside=nside, map_per_cl=maps_per_cl, \n",
    "                             noise_maps=noise_maps, beam_w=2*res, kind_of_map=kind_of_map, \n",
    "                             raw=0 , n_channels=n_channels,beam_yes=1 , verbose=0)\n",
    "\n",
    "\n",
    "x_train,y_train,x_val,y_val = nuf.prepare_data(y_r,mappe_B,r,n_train,n_train_fix,fval,maps_per_cl\n",
    "                                               , batch_size, batch_ordering=batch_ordering)\n",
    "\n",
    "if norm:\n",
    "    y_train=nuf.normalize_data(y_train,r)\n",
    "    y_val=nuf.normalize_data(y_val,r)\n",
    "#np.savez(base_dir+\"check_r_distribution\",y_train=y_train,y_val=y_val) \n",
    "#rand_indexes=np.random.randint(0,len(y_train)-1,10000)\n",
    "#np.savez(base_dir+\"check_train_maps\",y_train=y_train[rand_indexes], x_train=x_train[rand_indexes])\n",
    "\n",
    "if map_norm:\n",
    "    for i in range(len(x_train)):\n",
    "        for j in range(n_inputs):\n",
    "            x=x_train[i,:,j]\n",
    "            x_train[i,:,j]=nuf.normalize_data(x,x)\n",
    "    for i in range(len(x_val)):\n",
    "        for j in range(n_inputs):\n",
    "            x=x_val[i,:,j]\n",
    "            x_val[i,:,j]=nuf.normalize_data(x,x)\n",
    "f_train=np.load(base_dir_tau+\"predictions.npz\")\n",
    "normalizer=f_train[\"norm\"]\n",
    "model_tau = keras.models.load_model(base_dir_tau+test_model_folder) \n",
    "predictions_train=model_tau.predict(x_train)\n",
    "predictions_val=model_tau.predict(x_val)\n",
    "\n",
    "if norm:\n",
    "    y_train_sigma=(predictions_train-y_train)**2 * np.std(r)**2\n",
    "    y_val_sigma=(predictions_val-y_val)**2 * np.std(r)**2\n",
    "    count,red=uf.check_y(y_train_sigma)\n",
    "    y_train_sigma=nuf.normalize_data(y_train_sigma,red)\n",
    "    y_val_sigma=nuf.normalize_data(y_val_sigma,red)\n",
    "else:\n",
    "    y_train_sigma=(predictions_train-y_train)**2 \n",
    "    y_val_sigma=(predictions_val-y_val)**2 \n",
    "    red=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3072, 2)]         0         \n",
      "                                                                 \n",
      " order_map (OrderMap)        (None, 27648, 2)          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 3072, 32)          576       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3072, 32)          0         \n",
      "                                                                 \n",
      " order_map_1 (OrderMap)      (None, 3072, 32)          0         \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  (None, 768, 32)          0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " order_map_2 (OrderMap)      (None, 6912, 32)          0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 768, 32)           9216      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 768, 32)           0         \n",
      "                                                                 \n",
      " order_map_3 (OrderMap)      (None, 768, 32)           0         \n",
      "                                                                 \n",
      " average_pooling1d_1 (Averag  (None, 192, 32)          0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " order_map_4 (OrderMap)      (None, 1728, 32)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 192, 32)           9216      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 192, 32)           0         \n",
      "                                                                 \n",
      " order_map_5 (OrderMap)      (None, 192, 32)           0         \n",
      "                                                                 \n",
      " average_pooling1d_2 (Averag  (None, 48, 32)           0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " order_map_6 (OrderMap)      (None, 432, 32)           0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 48, 32)            9216      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 48, 32)            0         \n",
      "                                                                 \n",
      " order_map_7 (OrderMap)      (None, 48, 32)            0         \n",
      "                                                                 \n",
      " average_pooling1d_3 (Averag  (None, 12, 32)           0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 32)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 384)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 48)                18480     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 48)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 49        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,753\n",
      "Trainable params: 46,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_sigma=model_tau\n",
    "print(model_sigma.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.initializers.initializers_v2.GlorotUniform object at 0x7f742805ff10>\n",
      "<keras.initializers.initializers_v2.GlorotUniform object at 0x7f72a6a46bb0>\n"
     ]
    }
   ],
   "source": [
    "#print(model_sigma.layers[-3].get_weights()[0].shape)\n",
    "print(model_sigma.layers[2].kernel_initializer)\n",
    "print(model_sigma.layers[-3].kernel_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13393912 -0.01332623  0.01199918  0.0371391  -0.05096084  0.02500285\n",
      "  0.03696605 -0.07588221 -0.03077154  0.01558946  0.06996562 -0.0685127\n",
      "  0.07681809  0.04902276  0.09094316  0.05875527  0.00512966 -0.03820913\n",
      "  0.04891978  0.13494642 -0.09406272  0.04546189  0.0360902  -0.00572138\n",
      "  0.07928275 -0.08305775 -0.09737176  0.08246203  0.01632251  0.102522\n",
      " -0.12621152  0.00500077 -0.09085448 -0.01754452 -0.0089463  -0.10991067\n",
      "  0.11008615  0.09613534 -0.01538811 -0.04439099 -0.07076667 -0.05878779\n",
      " -0.1227631   0.0476659   0.07127575  0.05245948 -0.02936278 -0.05320385]\n",
      "[ 3.0249076e-02  1.0899892e-01 -9.4232619e-02 -1.2615614e-01\n",
      " -8.5225940e-02  1.3143587e-01  6.6460796e-02 -1.9525122e-02\n",
      "  3.9146092e-02 -3.5023730e-02  2.4077099e-02  6.0327891e-02\n",
      " -7.3515467e-02  4.3650016e-02 -5.3572603e-02  1.0712527e-01\n",
      " -1.6575560e-02  2.5486732e-02  7.3141709e-02  5.0883669e-02\n",
      " -1.2890819e-01  6.3277848e-02 -7.3917340e-03 -1.3200235e-01\n",
      " -5.3141834e-03 -3.2194648e-02 -9.8801367e-02  1.1823927e-01\n",
      " -1.3256288e-01  4.0849846e-02 -7.8314327e-02 -3.4595318e-02\n",
      " -4.0742126e-03  2.4926752e-02 -4.7993481e-02 -4.7816437e-02\n",
      " -7.4436456e-02  3.7436660e-02  3.7524171e-02  3.7670259e-03\n",
      " -1.2659035e-02 -1.4118330e-01  9.7911805e-03 -3.5106448e-05\n",
      " -9.4785891e-02  6.7789465e-02 -1.3187145e-01 -3.3564414e-03]\n",
      "Epoch 1/2\n",
      "5/5 - 1s - loss: 1.0458 - val_loss: 1.4399 - lr: 0.0010 - 1s/epoch - 297ms/step\n",
      "Epoch 2/2\n",
      "5/5 - 0s - loss: 1.0449 - val_loss: 1.4385 - lr: 0.0010 - 141ms/epoch - 28ms/step\n",
      "[ 2.93237288e-02  1.10971585e-01 -9.27031264e-02 -1.27131343e-01\n",
      " -8.57159942e-02  1.31435871e-01  6.94928840e-02 -1.86373014e-02\n",
      "  3.91460918e-02 -3.39180678e-02  2.38064472e-02  6.03278913e-02\n",
      " -7.39666969e-02  4.61780950e-02 -5.51577024e-02  1.07883006e-01\n",
      " -1.61471441e-02  2.85372511e-02  7.58351311e-02  5.28930649e-02\n",
      " -1.28908187e-01  5.91849014e-02 -7.03861937e-03 -1.28631800e-01\n",
      " -6.05250709e-03 -3.25544141e-02 -1.04405731e-01  1.17755756e-01\n",
      " -1.36385947e-01  4.18885499e-02 -7.62298629e-02 -2.98804082e-02\n",
      "  1.65008893e-03  2.08120160e-02 -4.56957780e-02 -5.17919399e-02\n",
      " -7.44364560e-02  3.33272889e-02  3.49261202e-02 -5.58924949e-05\n",
      " -9.94803850e-03 -1.47715151e-01  1.22547988e-02 -4.15328844e-03\n",
      " -1.00253008e-01  6.51809871e-02 -1.33420318e-01 -2.05477525e-04]\n",
      "Epoch 1/2\n",
      "5/5 - 2s - loss: 1.0444 - val_loss: 1.4385 - lr: 3.0000e-04 - 2s/epoch - 314ms/step\n",
      "Epoch 2/2\n",
      "5/5 - 0s - loss: 1.0443 - val_loss: 1.4385 - lr: 3.0000e-04 - 142ms/epoch - 28ms/step\n",
      "3/3 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/amorelli/pipeline/test_double/sigma/test_model_sigma/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/amorelli/pipeline/test_double/sigma/test_model_sigma/assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import initializers\n",
    "model_sigma.trainable=True\n",
    "set_layer=False\n",
    "#print(model_sigma.layers[-3].get_weights()[0][0])\n",
    "for layer in model_sigma.layers:\n",
    "    if layer.name == \"flatten\":\n",
    "        set_layer=True\n",
    "    if set_layer:\n",
    "        layer.trainable = True\n",
    "        if isinstance(layer, keras.layers.Dense):\n",
    "            w_shape,b_shape = (layer.get_weights()[0].shape,layer.get_weights()[1].shape)\n",
    "            w_initializer = tf.keras.initializers.GlorotUniform()\n",
    "            w_values = w_initializer(shape=w_shape)\n",
    "            b_initializer = tf.keras.initializers.Zeros()\n",
    "            b_values = b_initializer(shape=b_shape)\n",
    "            layer.set_weights([w_values,b_values])\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "#print(model_sigma.layers[-3].get_weights()[0][0])\n",
    "nuf.compile_and_fit(model_sigma, x_train, y_train_sigma, x_val, y_val_sigma, batch_size, max_epochs[0], stopping_monitor,p_stopping[0],\n",
    "                    reduce_monitor,f_reduce[0], p_reduce[0],base_dir, \n",
    "                    loss_training,lr[0],metrics,shuffle=True,verbose=2,callbacks=[True,True,True,True],append=False)\n",
    "model_sigma.trainable=True\n",
    "set_layer=False\n",
    "for layer in model_sigma.layers:\n",
    "    if layer.name == \"conv1d_3\":\n",
    "        set_layer=True\n",
    "    if set_layer:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "nuf.compile_and_fit(model_sigma, x_train, y_train_sigma, x_val, y_val_sigma, batch_size, max_epochs[1], stopping_monitor,p_stopping[1],\n",
    "                    reduce_monitor,f_reduce[1], p_reduce[1],base_dir, \n",
    "                    loss_training,lr[1],metrics,shuffle=True,verbose=2,callbacks=[True,True,True,True],append=True)\n",
    "predictions_sigma=model_sigma.predict(x_train)\n",
    "np.savez(base_dir+\"predictions\",y_train=y_train_sigma, pred=predictions_sigma, norm=red)\n",
    "model_sigma.save(base_dir+'test_model_sigma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camb-kernel",
   "language": "python",
   "name": "camb-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
