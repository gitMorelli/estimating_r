{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import nnhealpix.layers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import metrics\n",
    "import keras\n",
    "import pandas as pd\n",
    "import os, shutil\n",
    "from loss_functions import sigma_loss, sigma_f_loss, sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics= [sigma_loss,sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma]\n",
    "metrics= [sigma_loss,sigma_batch_loss,mse_tau,mse_sigma,sigma_f_loss]\n",
    "home_dir=\"/home/amorelli/r_estimate/B_maps_white_noise/results_16_5_23/\"\n",
    "r_like=[0.0,0.001,0.005,0.007,0.01] #these are the r_test of the maps on which i test the NN \n",
    "sigma_exact=[0.000225922*2, 0.000398311, 0.001017338,0.001142098,0.0015324*2] #these are the sigma computed for the \n",
    "#r_test using the likelihood approach\n",
    "load_dir='/home/amorelli/r_estimate/B_maps_white_noise/' \n",
    "to_load=['test_data_r0000_t006_150.npz','test_data_r0001_t006_15.npz','test_data_r0005_t006_241.npz',\n",
    "         'test_data_r0007_t006_241.npz','test_data_r0010_t006_24.npz'] #these are the test maps\n",
    "loss_kind=\"sigma\" #var\n",
    "loss_training=sigma_batch_loss \n",
    "test_model_folder=\"test_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name             loss  noise  p_stopping  p_reduce  f_reduce  \\\n",
      "0  16_5_23  new_sigma_batch      4          20         5       0.5   \n",
      "\n",
      "              stop-reduce      lr  batch_size  n_layers  nodes_layers comments  \n",
      "0  val_mse_batch-val_loss  0.0003          32      True            48           \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    hyperparameters=pd.read_csv(home_dir+\"output.txt\",delim_whitespace=True, index_col=None) #i try to read the \n",
    "    #file with the hyperparameters used in the training\n",
    "except:\n",
    "    print(\"no file to read\") \n",
    "    dataframe={}\n",
    "    dataframe[\"no data\"]=[\"-\"]\n",
    "    hyperparameters=pd.DataFrame(dataframe)#if there is no file to read (because i analyse an old model) i create an \n",
    "    #empty dataframe\n",
    "finally:\n",
    "    None #i use this so that the program continue after the try,except\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[]\n",
    "for name in to_load: \n",
    "    files.append(np.load(load_dir+name) )\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "for i,f in enumerate(files):#i take the maps and the corresponding r_test for each input file\n",
    "    x_test.append(f[\"x_test\"])\n",
    "    y=np.zeros(len(x_test[i]))+f[\"y_test\"] #y_test is a single number in the file -> i need to transform it in an array\n",
    "    #of the same length of x_test\n",
    "    y_test.append(y.reshape(y.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "313/313 - 7s - loss: 1.8331e-06 - sigma_loss: 4.0980e-06 - sigma_batch_loss: 1.8313e-06 - mse_tau: 1.7760e-06 - mse_sigma: 2.3220e-06 - sigma_f_loss: 232.2026 - 7s/epoch - 22ms/step\n",
      "313/313 [==============================] - 7s 20ms/step\n",
      "313/313 - 6s - loss: 5.5308e-06 - sigma_loss: 1.1123e-05 - sigma_batch_loss: 5.5259e-06 - mse_tau: 5.2251e-06 - mse_sigma: 5.8978e-06 - sigma_f_loss: 589.7819 - 6s/epoch - 19ms/step\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 - 6s - loss: 4.9085e-05 - sigma_loss: 8.1006e-05 - sigma_batch_loss: 4.9058e-05 - mse_tau: 4.7225e-05 - mse_sigma: 3.3782e-05 - sigma_f_loss: 3378.1519 - 6s/epoch - 19ms/step\n",
      "313/313 [==============================] - 6s 19ms/step\n",
      "313/313 - 6s - loss: 3.7026e-05 - sigma_loss: 6.8996e-05 - sigma_batch_loss: 3.7010e-05 - mse_tau: 3.6281e-05 - mse_sigma: 3.2715e-05 - sigma_f_loss: 3271.5105 - 6s/epoch - 19ms/step\n",
      "313/313 [==============================] - 6s 20ms/step\n",
      "313/313 - 6s - loss: 4.9930e-05 - sigma_loss: 7.4777e-05 - sigma_batch_loss: 4.9903e-05 - mse_tau: 4.7008e-05 - mse_sigma: 2.7769e-05 - sigma_f_loss: 2776.9180 - 6s/epoch - 20ms/step\n",
      "313/313 [==============================] - 7s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\n",
    "    home_dir+test_model_folder,  custom_objects={'loss_training' : loss_training, 'metrics' : metrics}, \n",
    "    compile=False\n",
    ") #i restore the model from the test_model folder. I need to specify the custom objects and recompile the model with the custom\n",
    "#objects, thus the metrics and the loss functions\n",
    "model.compile(loss=loss_training,optimizer=tf.optimizers.Adam(), metrics=metrics)\n",
    "\n",
    "results=[]\n",
    "predictions=[]\n",
    "for x,y in zip(x_test,y_test): #i compute the predictions of the model for each test_set and save them in a dict\n",
    "    results.append(model.evaluate(x,y,verbose=2) )\n",
    "    predictions.append(model.predict(x))\n",
    "npz_dict={}\n",
    "for i,p in enumerate(predictions):\n",
    "    npz_dict[f\"sigma_{r_like[i]}\"]=p[:,1]\n",
    "np.savez(home_dir+\"sigma_out\", **npz_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00026497 0.00026497 0.00026497 ... 0.00026497 0.00026497 0.00026497]\n"
     ]
    }
   ],
   "source": [
    "print(npz_dict[f\"sigma_{r_like[0]}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_kind==\"sigma\": #if the output of the NN is directly the std i do some operations and print some values, if\n",
    "    #it gives the variance i do in a different way\n",
    "    r_pred=[]\n",
    "    sigma_pred=[]\n",
    "    r_est=[]\n",
    "    sigma_r=[]\n",
    "    sigma_est=[]\n",
    "    sigma_sigma=[]\n",
    "    mae_r=[]\n",
    "    skeweness_r=[]\n",
    "    skeweness_sigma=[]\n",
    "    median_r=[]\n",
    "    median_sigma=[]\n",
    "    for i,p in enumerate(predictions): #i compute mean, std, etc for each set of predictions and save them in the corresponding\n",
    "        #dictionaries (see above)\n",
    "        r_pred.append(p[:,0])\n",
    "        sigma_pred.append(p[:,1])\n",
    "        r_est.append(np.mean(r_pred[i]))\n",
    "        sigma_r.append(np.std(r_pred[i]))\n",
    "        sigma_est.append(np.mean(sigma_pred[i]))\n",
    "        sigma_sigma.append(np.std(sigma_pred[i]))\n",
    "        mae_r.append(np.sum(np.abs(r_pred[i]-r_est[i]))/len(r_pred[i]))\n",
    "        skeweness_r.append(np.sum(((r_pred[i]-r_est[i])/sigma_r[i])**3)/len(r_pred[i]))\n",
    "        skeweness_sigma.append(np.sum(((sigma_pred[i]-sigma_est[i])/sigma_sigma[i])**3)/len(r_pred[i]))\n",
    "        median_r.append(np.median(r_pred[i]))\n",
    "        median_sigma.append(np.median(sigma_pred[i]))\n",
    "    #r_test=np.asarray([int(x*100)/100 for x in r_est])\n",
    "    #sigma_like=np.empty_like(r_test,dtype=float)\n",
    "    #for r in r_test:\n",
    "        #i=r_like.index(r)\n",
    "        #sigma_like[i]=sigma_exact[i]\n",
    "    d={}# i create a dictionary that stores the value of the following keywords for each test_set\n",
    "    d[\"r_test\"]=r_like\n",
    "    d[\"r_est\"]=r_est\n",
    "    d[\"dr\"]=np.abs(np.asarray(r_like)-np.asarray(r_est))/r_est\n",
    "    d[\"median_r-r_est\"]=np.asarray(median_r)-np.asarray(r_est)\n",
    "    d[\"mae_r\"]=mae_r\n",
    "    d[\"skeweness_r\"]=skeweness_r\n",
    "    d[\"sigma_r\"]=sigma_r\n",
    "    d[\"sigma_est\"]=sigma_est\n",
    "    d[\"dsigma\"]=np.abs(np.asarray(sigma_r)-np.asarray(sigma_est))/sigma_est\n",
    "    d[\"dlike\"]=np.abs(np.asarray(sigma_r)-np.asarray(sigma_exact))/sigma_r\n",
    "    d[\"sigma_sigma\"]=sigma_sigma\n",
    "    d[\"median_sigma-sigma_est\"]=np.asarray(median_sigma)-np.asarray(sigma_est)\n",
    "    d[\"skeweness_sigma\"]=skeweness_sigma\n",
    "\n",
    "    r_data=pd.DataFrame(d)\n",
    "\n",
    "    #define new DataFrame as original DataFrame with each row repeated 3 times. I do this because i want to join the hyperpar. dataframe\n",
    "    #with the r_data dataframe and the hyperparam. dataframe only has one row\n",
    "    hp_new = pd.DataFrame(np.repeat(hyperparameters.values, len(r_est), axis=0))\n",
    "    #assign column names of original DataFrame to new DataFrame\n",
    "    hp_new.columns = hyperparameters.columns\n",
    "\n",
    "    out_df = pd.concat([hp_new, r_data], axis=1) # i join the two dataframes so that i can copy-paste all the info for one model directly\n",
    "    #in excel\n",
    "\n",
    "    print(out_df)\n",
    "    out_df.to_csv(home_dir+'results.txt', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_kind==\"sigma\": \n",
    "    for i in range(len(r_pred)):\n",
    "        print(\"on_test_set r =\",r_like[i])\n",
    "        print(\"r_est:\",r_est[i],\"  This is the average of the $r_{i}^{NN}$ estimated by the network on the test set.\")\n",
    "        print(\"sigma_r:\",sigma_r[i], \" This is the std of the $r_i^{NN}$ array (std of the prediction).\")\n",
    "        print(\"sigma_r_est:\", sigma_est[i], \"  This is the average of the $\\sigma_{i}$ estimated by the network on the test set.\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_kind==\"sigma\":\n",
    "    fig, ax = plt.subplots(len(r_pred),2, figsize = (12,20))\n",
    "    for i in range(len(r_pred)): #for each test_set i create an histogram of the r_pred and sigma_pred and print both \n",
    "        bin_size=100\n",
    "        bin_edges= np.histogram_bin_edges(r_pred[i], bins='fd')\n",
    "        print(\"n_of_bins:\",len(bin_edges))\n",
    "        counts_tau, bins_tau = np.histogram(r_pred[i], bins=bin_edges) \n",
    "        bin_edges= np.histogram_bin_edges(sigma_pred[i], bins='fd')\n",
    "        counts_sigma, bins_sigma = np.histogram(sigma_pred[i], bins=bin_edges)\n",
    "        plt.subplot(len(r_pred),2,2*i+1)\n",
    "        plt.stairs(counts_tau, bins_tau, label=\"r\",  color='k')\n",
    "        plt.axvline(r_like[i], color=\"black\", linestyle=\"--\", label=\"r_test\")\n",
    "        #plt.axvline(median_r[i], color=\"black\", linestyle=\"--\", label=\"r_test\")\n",
    "        plt.axvline(r_est[i], color=\"red\", linestyle=\"-\", label=\"r_est\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"$r$\")\n",
    "        plt.ylabel(\"$counts$\")\n",
    "        plt.ticklabel_format(style='sci',useOffset=True, axis='x', scilimits=[-1, 1])\n",
    "        plt.subplot(len(r_pred),2,2*i+2)\n",
    "        plt.stairs(counts_sigma, bins_sigma, label=\"std estimated by NN\",  color='k')\n",
    "        plt.axvline(sigma_r[i], color=\"black\", linestyle=\"--\", label=\"std on the r estimated by NN\")\n",
    "        plt.axvline(sigma_est[i], color=\"red\", linestyle=\"-\", label=\"Average of the std estimated by NN\")\n",
    "        plt.legend()\n",
    "        plt.ticklabel_format(style='sci',useOffset=True, axis='x', scilimits=[-1, 1])\n",
    "        plt.xlabel(\"$\\sigma_{r}^{NN}$\")\n",
    "        plt.ylabel(\"$counts$\")\n",
    "        plt.legend()\n",
    "        #print(counts_tau, bins_tau)\n",
    "        #print(np.sort(r_pred[i]))\n",
    "    #plt.savefig('results_on_test.jpg') #must be in same cell or result image will be blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_kind!=\"sigma\":\n",
    "    r_pred=[]\n",
    "    var_pred=[]\n",
    "    r_est=[]\n",
    "    var_r=[]\n",
    "    var_est=[]\n",
    "    sigma_var=[]\n",
    "    mae_r=[]\n",
    "    skeweness_r=[]\n",
    "    skeweness_var=[]\n",
    "    median_r=[]\n",
    "    median_var=[]\n",
    "    for i,p in enumerate(predictions):\n",
    "        r_pred.append(p[:,0])\n",
    "        var_pred.append(p[:,1])\n",
    "        r_est.append(np.mean(r_pred[i]))\n",
    "        var_r.append(np.std(r_pred[i])**2)\n",
    "        var_est.append(np.mean(var_pred[i]))\n",
    "        sigma_var.append(np.std(var_pred[i]))\n",
    "        mae_r.append(np.sum(np.abs(r_pred[i]-r_est[i]))/len(r_pred[i]))\n",
    "        skeweness_r.append(np.sum(((r_pred[i]-r_est[i])/sigma_r[i])**3)/len(r_pred[i]))\n",
    "        skeweness_var.append(np.sum(((var_pred[i]-var_est[i])/sigma_var[i])**3)/len(r_pred[i]))\n",
    "        median_r.append(np.median(r_pred[i]))\n",
    "        median_var.append(np.median(var_pred[i]))\n",
    "    r_test=np.asarray([int(x*100) for x in r_est])\n",
    "    var_like=np.empty_like(r_test)\n",
    "    for r in r_test:\n",
    "        i=r_like.index(r)\n",
    "        var_like[i]=sigma_exact[i]**2\n",
    "    d[\"r_test\"]=r_test\n",
    "    d[\"r_est\"]=r_est\n",
    "    d[\"dr\"]=np.abs(r_test-r_est)/r_est\n",
    "    d[\"median_r-r_est\"]=median_r-r_est\n",
    "    d[\"mae_r\"]=mae_r\n",
    "    d[\"skeweness_r\"]=skeweness_r\n",
    "    sigma_r=var_r**{0.5}\n",
    "    d[\"sigma_r\"]=sigma_r\n",
    "    sigma_est=var_est**{0.5}\n",
    "    d[\"sigma_est\"]=sigma_est\n",
    "    d[\"dsigma\"]=np.abs(sigma_r-sigma_est)/sigma_est\n",
    "    d[\"dlike\"]=np.abs(sigma_r-sigma_like)/sigma_r\n",
    "    d[\"sigma_var\"]=sigma_var\n",
    "    d[\"median_var-var_est\"]=median_var-var_est\n",
    "    d[\"skeweness_var\"]=skeweness_var\n",
    "\n",
    "    r_data=pd.DataFrame(d)\n",
    "\n",
    "    #define new DataFrame as original DataFrame with each row repeated 3 times\n",
    "    hp_new = pd.DataFrame(np.repeat(hyperparameters.values, len(r_est), axis=0))\n",
    "\n",
    "    #assign column names of original DataFrame to new DataFrame\n",
    "    hp_new.columns = hyperparameters.columns\n",
    "\n",
    "    out_df = pd.concat([hp_new, r_data], axis=1)\n",
    "\n",
    "    print(out_df)\n",
    "    out_df.to_csv(base_dir+'results.txt', index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_kind!=\"sigma\":\n",
    "    for i in range(len(r_pred)):\n",
    "        print(\"r_est:\",r_est[i],\"  This is the average of the $r_{i}^{NN}$ estimated by the network on the test set.\")\n",
    "        print(\"var_r:\",sigma_r[i], \" This is the variance of the $r_i^{NN}$ array (variance of the prediction).\", \"\\t The standard deviation is thus: \", np.sqrt(sigma_tau[i]))\n",
    "        print(\"var_r_est:\", sigma_r[i], \"  This is the average of the $\\sigma_{i}$ estimated by the network on the test set.\", \"\\t thus taking the squareroot:\",np.sqrt(sigma_est[i]), \"We call it like that because $\\sigma_{i}$ should be an estimate of the variance in this case → its squareroot should be an estimate of the standard deviation\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_kind!=\"sigma\":\n",
    "    fig, ax = plt.subplots(len(r_pred),2, figsize = (10,10))\n",
    "    for i in range(len(r_pred)):\n",
    "        counts_tau, bins_tau = np.histogram(r_pred[i])\n",
    "        counts_sigma, bins_sigma = np.histogram(sigma_pred[i])\n",
    "        plt.subplot(len(r_pred),2,2*i+1)\n",
    "        ax[2*i+1].stairs(counts_tau, bins_tau, label=\"r\",  color='k')\n",
    "        plt.axvline(r_test[i], color=\"black\", linestyle=\"--\", label=\"r_test\")\n",
    "        plt.axvline(r_est[i], color=\"red\", linestyle=\"-\", label=\"r_est\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"$r$\")\n",
    "        plt.ylabel(\"$counts$\")\n",
    "        plt.subplot(len(r_pred),2,2*i+2)\n",
    "        ax[2*i+2].stairs(counts_sigma, bins_sigma, label=\"std estimated by NN\",  color='k')\n",
    "        plt.axvline(sigma_r[i], color=\"black\", linestyle=\"--\", label=\"std on the r estimated by NN\")\n",
    "        plt.axvline(sigma_est[i], color=\"red\", linestyle=\"-\", label=\"Average of the std estimated by NN\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"$\\sigma_{r}^{NN}$\")\n",
    "        plt.ylabel(\"$counts$\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camb-kernel",
   "language": "python",
   "name": "camb-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
