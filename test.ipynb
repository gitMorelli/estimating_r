{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "#i import the necessary libraries\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import nnhealpix.layers\n",
    "from tensorflow.keras import metrics\n",
    "import pandas as pd\n",
    "from loss_functions import sigma_loss, sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma, mse_batch, sigma_f_loss\n",
    "import math\n",
    "import useful_functions as uf\n",
    "import NN_functions as nuf\n",
    "\n",
    "seed_train=33\n",
    "np.random.seed(seed_train)# i set a random seed for the generation of the maps for reproducibility\n",
    "\n",
    "#map gen\n",
    "nside = 16\n",
    "n_train=300 #the total number of training+validation pair of maps that i will generate\n",
    "n_train_fix=40 #the total number of of training maps i will spread on all the r interval -> for each r value i generate n_train_fix/len(r) maps \n",
    "kind_of_map=\"QU\"\n",
    "n_channels=2\n",
    "pol=2\n",
    "res=hp.nside2resol(nside, arcmin=False) \n",
    "sensitivity=4\n",
    "\n",
    "name='2_6_23'\n",
    "base_dir='/home/amorelli/QU_foreground_tau/'+name+'/'\n",
    "# callbacks\n",
    "reduce_lr_on_plateau = True\n",
    "p_stopping=20\n",
    "p_reduce=5\n",
    "f_reduce=0.5\n",
    "stopping_monitor=\"val_mse_batch\"\n",
    "reduce_monitor=\"val_loss\"\n",
    "metrics=[sigma_loss, sigma_batch_loss,mse_tau,mse_sigma, sigma_f_loss, mse_batch]# these are the different loss functions i have used. I use them as metrics\n",
    "\n",
    "#network structure\n",
    "one_layer=True # this is to switch between one dense layer or two dense layer\n",
    "drop=0.2\n",
    "n_layer_0=48\n",
    "n_layer_1=64\n",
    "n_layer_2=16\n",
    "if kind_of_map!=\"QU\": \n",
    "    n_inputs=n_channels\n",
    "else:\n",
    "    n_inputs=pol*n_channels\n",
    "\n",
    "#train and val\n",
    "batch_size = 16\n",
    "max_epochs = 200\n",
    "lr=0.0003 \n",
    "fval=0.25 # this is the fraction of data that i use for validation, computed on n_train_fix\n",
    "training_loss=\"new_sigma_batch\"\n",
    "loss_training=sigma_batch_loss # this is the loss i use for the training\n",
    "shuffle=False\n",
    "\n",
    "f_ = np.load('/home/amorelli/cl_generator/outfile_l_47_complete.npz') \n",
    "#print(\"outfile_R:\",f_.files) #give the keiwords for the stored arrays\n",
    "labels=f_.files\n",
    "data=f_[labels[0]]\n",
    "r=f_[labels[1]]\n",
    "r, data=uf.unison_sorted_copies(r, data)\n",
    "#indexes=np.linspace(0,len(r)-1,10,dtype=int)\n",
    "#r=r[indexes]\n",
    "#data=data[indexes]\n",
    "\n",
    "input_folder=\"/home/amorelli/foreground_noise_maps/noise_generation\"\n",
    "input_files=[os.listdir(input_folder)[0]]\n",
    "for i in range(len(input_files)):\n",
    "    input_files[i]=input_folder+\"/\"+input_files[i]\n",
    "noise_maps=uf.generate_noise_maps(n_train,n_channels,nside,pol=2,sensitivity=sensitivity,input_files=input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3072, 4)\n"
     ]
    }
   ],
   "source": [
    "print(noise_maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camb-kernel",
   "language": "python",
   "name": "camb-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
