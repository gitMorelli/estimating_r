{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 - 5s - loss: 1.1300 - val_loss: 0.9996 - 5s/epoch - 1s/step\n",
      "Epoch 2/2\n",
      "5/5 - 0s - loss: 0.6746 - val_loss: 1.0021 - 363ms/epoch - 73ms/step\n",
      "3/3 [==============================] - 1s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "#i import the necessary libraries\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import nnhealpix.layers\n",
    "from tensorflow.keras import metrics\n",
    "import pandas as pd\n",
    "from loss_functions import sigma_loss, sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma, mse_batch, sigma_f_loss\n",
    "import math\n",
    "import useful_functions as uf\n",
    "import NN_functions as nuf\n",
    "import os, shutil\n",
    "\n",
    "seed_train=33\n",
    "np.random.seed(seed_train)# i set a random seed for the generation of the maps for reproducibility\n",
    "\n",
    "#map gen\n",
    "nside = 16\n",
    "n_train=100 #the total number of training+validation pair of maps that i will generate\n",
    "n_train_fix=100 #the total number of of training maps i will spread on all the r interval -> for each r value i generate n_train_fix/len(r) maps \n",
    "kind_of_map=\"BB\"\n",
    "n_channels=2\n",
    "pol=1\n",
    "res=hp.nside2resol(nside, arcmin=False) \n",
    "sensitivity=4\n",
    "\n",
    "name='results_11_6_23'\n",
    "base_dir='/home/amorelli/QU_foreground_tau/'+name+'/'\n",
    "# callbacks\n",
    "reduce_lr_on_plateau = True\n",
    "p_stopping=20\n",
    "p_reduce=5\n",
    "f_reduce=0.5\n",
    "stopping_monitor=\"val_loss\"\n",
    "reduce_monitor=\"val_loss\"\n",
    "metrics=[]# these are the different loss functions i have used. I use them as metrics\n",
    "\n",
    "#network structure\n",
    "one_layer=True # this is to switch between one dense layer or two dense layer\n",
    "drop=0.2\n",
    "n_layer_0=48\n",
    "n_layer_1=64\n",
    "n_layer_2=16\n",
    "if kind_of_map!=\"QU\": \n",
    "    n_inputs=n_channels\n",
    "else:\n",
    "    n_inputs=pol*n_channels\n",
    "\n",
    "#train and val\n",
    "batch_size = 16\n",
    "max_epochs = 2\n",
    "lr=0.0003 \n",
    "fval=0.1 # this is the fraction of data that i use for validation, computed on n_train_fix\n",
    "training_loss=\"mse tau\"\n",
    "loss_training=tf.keras.losses.MeanSquaredError() # this is the loss i use for the training\n",
    "shuffle=True\n",
    "norm=True\n",
    "\n",
    "f_ = np.load('/home/amorelli/cl_generator/outfile_R_000_001_seed=67.npz') \n",
    "#print(\"outfile_R:\",f_.files) #give the keiwords for the stored arrays\n",
    "labels=f_.files\n",
    "data=f_[labels[0]]\n",
    "r=f_[labels[1]]\n",
    "r, data=uf.unison_sorted_copies(r, data)\n",
    "indexes=np.linspace(0,len(r)-1,10,dtype=int)\n",
    "r=r[indexes]\n",
    "data=data[indexes]\n",
    "\n",
    "#input_folder=\"/home/amorelli/foreground_noise_maps/noise_generation\"\n",
    "#input_files=os.listdir(input_folder)\n",
    "#for i in range(len(input_files)):\n",
    "   # input_files[i]=input_folder+\"/\"+input_files[i]\n",
    "noise_maps=uf.generate_noise_maps(n_train,n_channels,nside,pol=1,sensitivity=sensitivity,input_files=None)\n",
    "\n",
    "#noise_E,noise_B=uf.convert_to_EB(noise_maps)\n",
    "\n",
    "maps_per_cl_gen=uf.maps_per_cl(distribution=0)\n",
    "maps_per_cl=maps_per_cl_gen.compute_maps_per_cl(r,n_train,n_train_fix)\n",
    "\n",
    "mappe_B,y_r=uf.generate_maps(data, r,n_train=n_train,nside=nside, map_per_cl=maps_per_cl, \n",
    "                             noise_maps=noise_maps, beam_w=2*res, kind_of_map=kind_of_map, \n",
    "                             raw=0 , n_channels=n_channels,beam_yes=1 , verbose=0)\n",
    "\n",
    "\n",
    "x_train,y_train,x_val,y_val = nuf.prepare_data(y_r,mappe_B,r,n_train,n_train_fix,fval,maps_per_cl\n",
    "                                               , batch_size, batch_ordering=False)\n",
    "\n",
    "if norm:\n",
    "    y_train=nuf.normalize_data(y_train,r)\n",
    "    y_val=nuf.normalize_data(y_val,r)\n",
    "#np.savez(base_dir+\"check_r_distribution\",y_train=y_train,y_val=y_val) \n",
    "\n",
    "#rand_indexes=np.random.randint(0,len(y_train)-1,10000)\n",
    "#np.savez(base_dir+\"check_train_maps\",y_train=y_train[rand_indexes], x_train=x_train[rand_indexes])\n",
    "\n",
    "model=nuf.build_network(n_inputs,nside,drop,n_layer_0,n_layer_1,n_layer_2,one_layer,\n",
    "                        num_output=1,use_normalization=[True,True],use_drop=False)\n",
    "\n",
    "history=nuf.compile_and_fit(model, x_train, y_train, x_val, y_val, batch_size, max_epochs, \n",
    "                            stopping_monitor,p_stopping,reduce_monitor,f_reduce, p_reduce,base_dir, \n",
    "                            loss_training,lr,metrics,shuffle=shuffle, verbose=2,callbacks=[False,False,False,False])\n",
    "\n",
    "#print('Saving model to disk')\n",
    "#model.save(base_dir+'test_model')\n",
    "\n",
    "predictions=model.predict(x_train)\n",
    "\n",
    "#np.savez(base_dir+\"predictions\",y_train=y_train, pred=predictions, norm=r)\n",
    "\n",
    "#-----------------------------------------\n",
    "hyperparameters={}\n",
    "hyperparameters[\"name\"]=name\n",
    "hyperparameters[\"loss\"]=training_loss\n",
    "hyperparameters[\"noise\"]=sensitivity\n",
    "hyperparameters[\"p_stopping\"]=p_stopping\n",
    "hyperparameters[\"p_reduce\"]=p_reduce\n",
    "hyperparameters[\"f_reduce\"]=f_reduce\n",
    "hyperparameters[\"stop-reduce\"]=stopping_monitor+\"-\"+reduce_monitor\n",
    "hyperparameters[\"lr\"]=lr\n",
    "hyperparameters[\"batch_size\"]=batch_size\n",
    "hyperparameters[\"n_layers\"]=one_layer\n",
    "if one_layer:\n",
    "    hyperparameters[\"nodes_layers\"]=n_layer_0\n",
    "else:\n",
    "    hyperparameters[\"nodes_layers\"]=str(n_layer_1)+\"-\"+str(n_layer_2)\n",
    "hyperparameters[\"comments\"]=\" \"\n",
    "hyperparameters = {k:[v] for k,v in hyperparameters.items()}\n",
    "output=pd.DataFrame(hyperparameters)\n",
    "#output.to_csv(base_dir+'output.txt', index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camb-kernel",
   "language": "python",
   "name": "camb-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
