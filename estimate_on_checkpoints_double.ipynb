{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 17:09:32.226163: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 17:09:33.508945: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-12 17:09:33.509075: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-12 17:09:33.509094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import nnhealpix.layers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import metrics\n",
    "import keras\n",
    "import pandas as pd\n",
    "import os, shutil\n",
    "import NN_functions as nuf\n",
    "from loss_functions import sigma_loss, sigma_f_loss, sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics= [sigma_loss,sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma]\n",
    "metrics= []\n",
    "home_dir=\"/home/amorelli/r_estimate/B_maps_white_noise/results_11_6_23/\"\n",
    "home_dir_sigma=\"/home/amorelli/r_estimate/B_maps_white_noise/results_double_test/\"\n",
    "\n",
    "#r_like=[0.0,0.01,0.03,0.05,0.06] #these are the r_test of the maps on which i test the NN \n",
    "#sigma_exact=[0.00011*2, 0.00152323164284316, 0.00286355246354494,0.00376209725434054,0.0024*2]\n",
    "#to_load=[\"test_data_r000_t006_24.npz\",\"test_data_r0010_t006_24.npz\",\"test_data_r0030_t006_24.npz\",\"test_data_r0050_t006_24.npz\",\"test_data_r0060_t006_54.npz\"]\n",
    "\n",
    "r_like=[0.0,0.001,0.005,0.007,0.01] #these are the r_test of the maps on which i test the NN \n",
    "sigma_exact=[0.000225922*2, 0.000398311, 0.001017338,0.001142098,0.0015324*2] #these are the sigma computed for the \n",
    "to_load=[\"test_data_r000_t006_77.npz\",\"test_data_r001_t006_78.npz\",\"test_data_r005_t006_79.npz\",\n",
    "         \"test_data_r007_t006_80.npz\",\"test_data_r0010_t006_81.npz\"] #these are the test maps\n",
    "\n",
    "#r_test using the likelihood approach\n",
    "#sigma_exact=[0.00073*2, 0.000398311, 0.00207732328746706,0.002335838427700651] tau 0.01,0.06,0.07,0.08\n",
    "#to_load=[\"test_data_005_40.npz\",\"test_data_007_40.npz\"]\n",
    "\n",
    "load_dir='/home/amorelli/r_estimate/B_maps_white_noise/' \n",
    "loss_training=tf.keras.losses.MeanSquaredError() \n",
    "checkpoint_dir=home_dir+\"checkpoints\"\n",
    "checkpoint_dir_sigma=home_dir_sigma+\"checkpoints\"\n",
    "last=[-1,-1]\n",
    "to_norm=True\n",
    "map_norm=False\n",
    "n_output=1\n",
    "in_shape=2\n",
    "n_layer_0=48\n",
    "n_layer_1=256\n",
    "n_layer_2=16\n",
    "nside=16\n",
    "drop=0.2\n",
    "one_layer=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               name            loss  noise  p_stopping  p_reduce  f_reduce  \\\n",
      "0  results_12b_6_23  sigma_log_loss      4          20         5       0.5   \n",
      "\n",
      "         stop-reduce      lr  batch_size  n_layers  nodes_layers comments  \n",
      "0  val_loss-val_loss  0.0003          16      True            48           \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    hyperparameters=pd.read_csv(home_dir+\"output.txt\",delim_whitespace=True, index_col=None) #i try to read the \n",
    "    #file with the hyperparameters used in the training\n",
    "except:\n",
    "    print(\"no file to read\") \n",
    "    dataframe={}\n",
    "    dataframe[\"no data\"]=[\"-\"]\n",
    "    hyperparameters=pd.DataFrame(dataframe)#if there is no file to read (because i analyse an old model) i create an \n",
    "    #empty dataframe\n",
    "finally:\n",
    "    None #i use this so that the program continue after the try,except\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train=np.load(home_dir+\"predictions.npz\")\n",
    "norm=f_train[\"norm\"]\n",
    "f_train_sigma=np.load(home_dir_sigma+\"predictions.npz\")\n",
    "norm_sigma=f_train_sigma[\"norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[]\n",
    "for name in to_load: \n",
    "    files.append(np.load(load_dir+name) )\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "for i,f in enumerate(files):#i take the maps and the corresponding r_test for each input file\n",
    "    x_test.append(f[\"x_test\"])\n",
    "    #y=np.zeros(len(x_test[i]))+f[\"y_test\"] #y_test is a single number in the file -> i need to transform it in an array\n",
    "    #of the same length of x_test\n",
    "    #y_test.append(y.reshape(y.shape[0],1))\n",
    "    y_test.append(f[\"y_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 17:09:41.224092: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-12 17:09:41.224190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (coka.fe.infn.it): /proc/driver/nvidia/version does not exist\n",
      "2023-06-12 17:09:41.225232: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model=nuf.build_network(in_shape,nside,drop,n_layer_0,n_layer_1,n_layer_2,one_layer, \n",
    "                        num_output=n_output,use_normalization=[False,False,False],use_drop=False)\n",
    "model.compile(loss=loss_training, optimizer=tf.optimizers.Adam(),metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=os.listdir(checkpoint_dir)\n",
    "files.sort()\n",
    "checkpoint_path=files[last[0]]\n",
    "model.load_weights(checkpoint_dir+'/'+ checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sigma=nuf.build_network(in_shape,nside,drop,n_layer_0,n_layer_1,n_layer_2,one_layer, \n",
    "                        num_output=n_output,use_normalization=[False,False,False],use_drop=False)\n",
    "model_sigma.compile(loss=loss_training, optimizer=tf.optimizers.Adam(),metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=os.listdir(checkpoint_dir_sigma)\n",
    "files.sort()\n",
    "checkpoint_path=files[last[1]]\n",
    "model_sigma.load_weights(checkpoint_dir_sigma+'/'+ checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 8s - loss: 189.8099 - sigma_loss: 366.3565 - sigma_batch_loss: 189.6586 - mse_tau: 91.7187 - mse_sigma: 274.6378 - sigma_f_loss: 27463780352.0000 - 8s/epoch - 27ms/step\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "0.00010684261 0.00015442637\n",
      "313/313 - 7s - loss: 126.2837 - sigma_loss: 177.8893 - sigma_batch_loss: 126.1838 - mse_tau: 56.9572 - mse_sigma: 120.9321 - sigma_f_loss: 12093208576.0000 - 7s/epoch - 22ms/step\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "0.0011645586 0.0003880479\n",
      "313/313 - 7s - loss: 18.9955 - sigma_loss: 11.7619 - sigma_batch_loss: 18.9766 - mse_tau: 3.9588 - mse_sigma: 7.8031 - sigma_f_loss: 780307136.0000 - 7s/epoch - 22ms/step\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "0.005304918 0.0009747296\n",
      "313/313 - 7s - loss: 63.2327 - sigma_loss: 58.2467 - sigma_batch_loss: 63.1874 - mse_tau: 20.2968 - mse_sigma: 37.9499 - sigma_f_loss: 3794984960.0000 - 7s/epoch - 22ms/step\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "0.0071177804 0.0009507828\n",
      "313/313 - 7s - loss: 131.2713 - sigma_loss: 177.9131 - sigma_batch_loss: 131.1649 - mse_tau: 53.2609 - mse_sigma: 124.6522 - sigma_f_loss: 12465213440.0000 - 7s/epoch - 22ms/step\n",
      "313/313 [==============================] - 7s 22ms/step\n",
      "0.00871778 0.00055644126\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "predictions=[]\n",
    "for x,y in zip(x_test,y_test): #i compute the predictions of the model for each test_set and save them in a dict\n",
    "    if map_norm:\n",
    "        for i in range(len(x)):\n",
    "            for j in range(in_shape):\n",
    "                t=x[i,:,j]\n",
    "                x[i,:,j]=nuf.normalize_data(t,t)\n",
    "    results.append(model.evaluate(x,y,verbose=2) )\n",
    "    pred_tau=model.predict(x)\n",
    "    pred_sigma=model_sigma.predict(x)\n",
    "    pred=np.ones(shape=(len(pred_tau),2))\n",
    "    if to_norm:\n",
    "        pred[:,0]=nuf.denormalize_data(pred_tau,norm)\n",
    "        pred[:,1]=nuf.denormalize_data(pred_sigma,norm_sigma)\n",
    "    predictions.append(pred)\n",
    "npz_dict={}\n",
    "for i,p in enumerate(predictions):\n",
    "    npz_dict[f\"sigma_{r_like[i]}\"]=p[:,1]\n",
    "np.savez(home_dir+\"sigma_out\", **npz_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tau_pred=predictions[:,0]\n",
    "sigma_pred=np.abs(predictions[:,1])#*predictions[:,1]\n",
    "tau_est=np.mean(tau_pred)\n",
    "sigma_tau=np.std(tau_pred)**2\n",
    "#sigma_tau=np.sum((tau_pred-0.06)**2)/10000\n",
    "sigma_est=np.mean(sigma_pred)\n",
    "counts_tau, bins_tau = np.histogram(tau_pred)\n",
    "counts_sigma, bins_sigma = np.histogram(sigma_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pred=[]\n",
    "var_pred=[]\n",
    "r_est=[]\n",
    "sigma_r=[]\n",
    "sigma_est=[]\n",
    "sigma_var=[]\n",
    "mae_r=[]\n",
    "skeweness_r=[]\n",
    "skeweness_var=[]\n",
    "median_r=[]\n",
    "median_var=[]\n",
    "for i,p in enumerate(predictions): #i compute mean, std, etc for each set of predictions and save them in the corresponding\n",
    "    #dictionaries (see above)\n",
    "    r_pred.append(p[:,0])\n",
    "    #var_pred.append(p[:,1])\n",
    "    var_pred.append(p[:,1])\n",
    "    r_est.append(np.mean(r_pred[i]))\n",
    "    sigma_r.append(np.std(r_pred[i]))\n",
    "    sigma_est.append(np.sqrt(np.mean(var_pred[i])))\n",
    "    sigma_var.append(np.std(var_pred[i]))\n",
    "    mae_r.append(np.sum(np.abs(r_pred[i]-r_est[i]))/len(r_pred[i]))\n",
    "    skeweness_r.append(np.sum(((r_pred[i]-r_est[i])/sigma_r[i])**3)/len(r_pred[i]))\n",
    "    skeweness_var.append(np.sum(((var_pred[i]-sigma_est[i]**2)/sigma_var[i])**3)/len(r_pred[i]))\n",
    "    median_r.append(np.median(r_pred[i]))\n",
    "    median_var.append(np.median(var_pred[i]))\n",
    "#r_test=np.asarray([int(x*100)/100 for x in r_est])\n",
    "#sigma_like=np.empty_like(r_test,dtype=float)\n",
    "#for r in r_test:\n",
    "    #i=r_like.index(r)\n",
    "    #sigma_like[i]=sigma_exact[i]\n",
    "d={}# i create a dictionary that stores the value of the following keywords for each test_set\n",
    "d[\"r_test\"]=r_like\n",
    "d[\"r_est\"]=r_est\n",
    "d[\"dr\"]=np.abs(np.asarray(r_like)-np.asarray(r_est))/r_est\n",
    "d[\"median_r-r_est\"]=np.asarray(median_r)-np.asarray(r_est)\n",
    "d[\"mae_r\"]=mae_r\n",
    "d[\"skeweness_r\"]=skeweness_r\n",
    "d[\"sigma_r\"]=sigma_r\n",
    "d[\"sigma_est\"]=sigma_est\n",
    "d[\"dsigma\"]=np.abs(np.asarray(sigma_r)-np.asarray(sigma_est))/sigma_est\n",
    "d[\"dlike\"]=np.abs(np.asarray(sigma_r)-np.asarray(sigma_exact))/sigma_r\n",
    "d[\"sigma_var\"]=sigma_var\n",
    "d[\"median_var-var_est\"]=np.asarray(median_var)-np.asarray(sigma_est)**2\n",
    "d[\"skeweness_var\"]=skeweness_var\n",
    "\n",
    "r_data=pd.DataFrame(d)\n",
    "\n",
    "#define new DataFrame as original DataFrame with each row repeated 3 times. I do this because i want to join the hyperpar. dataframe\n",
    "#with the r_data dataframe and the hyperparam. dataframe only has one row\n",
    "hp_new = pd.DataFrame(np.repeat(hyperparameters.values, len(r_est), axis=0))\n",
    "#assign column names of original DataFrame to new DataFrame\n",
    "hp_new.columns = hyperparameters.columns\n",
    "\n",
    "out_df = pd.concat([hp_new, r_data], axis=1) # i join the two dataframes so that i can copy-paste all the info for one model directly\n",
    "#in excel\n",
    "\n",
    "print(out_df)\n",
    "out_df.to_csv(home_dir+'results.txt', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(len(r_pred)):\n",
    "    print(\"on_test_set r =\",r_like[i])\n",
    "    print(\"r_est:\",r_est[i],\"  This is the average of the $r_{i}^{NN}$ estimated by the network on the test set.\")\n",
    "    print(\"sigma_r:\",sigma_r[i], \" This is the std of the $r_i^{NN}$ array (std of the prediction).\")\n",
    "    print(\"sigma_r_est:\", sigma_est[i], \"  This is the square root of the average of the $\\sigma_{i}$ estimated by the network on the test set. In this case the sigma_i is indeed an estimate of the variance\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(r_pred),2, figsize = (12,20))\n",
    "for i in range(len(r_pred)): #for each test_set i create an histogram of the r_pred and sigma_pred and print both \n",
    "    bin_size=100\n",
    "    bin_edges= np.histogram_bin_edges(r_pred[i], bins='fd')\n",
    "    print(\"n_of_bins:\",len(bin_edges))\n",
    "    counts_tau, bins_tau = np.histogram(r_pred[i], bins=bin_edges) \n",
    "    bin_edges= np.histogram_bin_edges(var_pred[i], bins='fd')\n",
    "    counts_sigma, bins_sigma = np.histogram(var_pred[i], bins=bin_edges)\n",
    "    plt.subplot(len(r_pred),2,2*i+1)\n",
    "    plt.stairs(counts_tau, bins_tau, label=\"r\",  color='k')\n",
    "    plt.axvline(r_like[i], color=\"black\", linestyle=\"--\", label=\"r_test\")\n",
    "    #plt.axvline(median_r[i], color=\"black\", linestyle=\"--\", label=\"r_test\")\n",
    "    plt.axvline(r_est[i], color=\"red\", linestyle=\"-\", label=\"r_est\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"$r$\")\n",
    "    plt.ylabel(\"$counts$\")\n",
    "    plt.ticklabel_format(style='sci',useOffset=True, axis='x', scilimits=[-1, 1])\n",
    "    plt.subplot(len(r_pred),2,2*i+2)\n",
    "    plt.stairs(counts_sigma, bins_sigma, label=\"var estimated by NN\",  color='k')\n",
    "    plt.axvline(sigma_r[i]**2, color=\"black\", linestyle=\"--\", label=\"var on the r estimated by NN\")\n",
    "    plt.axvline(sigma_est[i]**2, color=\"red\", linestyle=\"-\", label=\"Average of the var estimated by NN\")\n",
    "    plt.legend()\n",
    "    plt.ticklabel_format(style='sci',useOffset=True, axis='x', scilimits=[-1, 1])\n",
    "    plt.xlabel(\"$(\\sigma_{r}^{NN})^2$\")\n",
    "    plt.ylabel(\"$counts$\")\n",
    "    plt.legend()\n",
    "    #print(counts_tau, bins_tau)\n",
    "    #print(np.sort(r_pred[i]))\n",
    "#plt.savefig('results_on_test.jpg') #must be in same cell or result image will be blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camb-kernel",
   "language": "python",
   "name": "camb-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
