{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 23:31:09.792087: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-02 23:31:13.855887: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-02 23:31:13.856206: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-02 23:31:13.856232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING: AstropyDeprecationWarning: \"verbose\" was deprecated in version 1.15.0 and will be removed in a future version.  [useful_functions]\n",
      "WARNING: AstropyDeprecationWarning: \"verbose\" was deprecated in version 1.15.0 and will be removed in a future version.  [useful_functions]\n",
      "WARNING: AstropyDeprecationWarning: \"verbose\" was deprecated in version 1.15.0 and will be removed in a future version.  [useful_functions]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 23:31:19.731295: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-07-02 23:31:19.732514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (coka.fe.infn.it): /proc/driver/nvidia/version does not exist\n",
      "2023-07-02 23:31:19.733565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import nnhealpix.layers\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "from loss_functions import sigma_loss, sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma, mse_batch, sigma_f_loss\n",
    "import math\n",
    "import useful_functions as uf\n",
    "import NN_functions as nuf\n",
    "import os, shutil\n",
    "\n",
    "seed_train=4\n",
    "np.random.seed(seed_train)# i set a random seed for the generation of the maps for reproducibility\n",
    "tf.random.set_seed(seed_train)\n",
    "\n",
    "#map gen\n",
    "nside = 16\n",
    "n_train=100 #the total number of training+validation pair of maps that i will generate\n",
    "n_train_fix=40 #the total number of of training maps i will spread on all the r interval -> for each r value i generate n_train_fix/len(r) maps \n",
    "kind_of_map=\"EE\"\n",
    "n_channels=2\n",
    "pol=1\n",
    "res=hp.nside2resol(nside, arcmin=False) \n",
    "sensitivity=4\n",
    "\n",
    "base_dir='/home/amorelli/E_foreground/29_6_23/'\n",
    "base_dir_tau='/home/amorelli/E_foreground/25_6_23/'\n",
    "test_model_folder=\"test_model\"\n",
    "\n",
    "# callbacks\n",
    "p_stopping=20\n",
    "p_reduce=5\n",
    "f_reduce=0.5\n",
    "stopping_monitor=\"val_loss\"\n",
    "reduce_monitor=\"val_loss\"\n",
    "metrics=[]#\n",
    "metrics_tau=[sigma_loss, sigma_batch_loss,mse_tau,mse_sigma, sigma_f_loss, mse_batch]\n",
    "\n",
    "#network structure\n",
    "drop=[0.2,0.2,0.2]\n",
    "activation_dense=\"relu\"\n",
    "kernel_initializer=\"glorot_uniform\"\n",
    "use_drop=[False,True,True]\n",
    "use_normalization=[False,False,False]\n",
    "n_layers=1\n",
    "nodes_per_layer=[64,256,256]\n",
    "if kind_of_map!=\"QU\": \n",
    "    n_inputs=n_channels\n",
    "else:\n",
    "    n_inputs=pol*n_channels\n",
    "n_output=1\n",
    "n_output_tau=2\n",
    "\n",
    "#train and val\n",
    "#train and val\n",
    "batch_size = 16\n",
    "max_epochs = 200\n",
    "lr=0.0001 \n",
    "fval=0.1 # this is the fraction of data that i use for validation, computed on n_train_fix\n",
    "training_loss=\"mse\"\n",
    "loss_training=tf.keras.losses.MeanSquaredError() # this is the loss i use for the training\n",
    "loss_training_tau=sigma_batch_loss\n",
    "shuffle=True\n",
    "norm=True\n",
    "map_norm=True\n",
    "batch_ordering=False\n",
    "distr=0\n",
    "n_optimizer=0\n",
    "callbacks=[True,True,True,True,False]\n",
    "\n",
    "f_ = np.load('/home/amorelli/cl_generator/outfile_l_47_complete.npz') \n",
    "#print(\"outfile_R:\",f_.files) #give the keiwords for the stored arrays\n",
    "labels=f_.files\n",
    "data=f_[labels[0]]\n",
    "r=f_[labels[1]]\n",
    "r, data=uf.unison_sorted_copies(r, data)\n",
    "indexes=np.linspace(0,len(r)-1,5,dtype=int)\n",
    "r=r[indexes]\n",
    "data=data[indexes]\n",
    "\n",
    "input_folder=\"/home/amorelli/foreground_noise_maps/noise_maps_d1s1_train\"\n",
    "input_files=os.listdir(input_folder)\n",
    "for i in range(len(input_files)):\n",
    "    input_files[i]=input_folder+\"/\"+input_files[i]\n",
    "noise_maps=uf.generate_noise_maps(n_train,n_channels,nside,pol=2,sensitivity=sensitivity,input_files=None)\n",
    "\n",
    "noise_E,noise_B=uf.convert_to_EB(noise_maps)\n",
    "\n",
    "maps_per_cl_gen=uf.maps_per_cl(distribution=distr)\n",
    "maps_per_cl=maps_per_cl_gen.compute_maps_per_cl(r,n_train,n_train_fix)\n",
    "\n",
    "mappe_B,y_r=uf.generate_maps(data, r,n_train=n_train,nside=nside, map_per_cl=maps_per_cl, \n",
    "                             noise_maps=noise_E, beam_w=2*res, kind_of_map=kind_of_map, \n",
    "                             raw=0 , n_channels=n_channels,beam_yes=1 , verbose=0)\n",
    "\n",
    "\n",
    "x_train,y_train,x_val,y_val = nuf.prepare_data(y_r,mappe_B,r,n_train,n_train_fix,fval,maps_per_cl\n",
    "                                               , batch_size, batch_ordering=batch_ordering)\n",
    "\n",
    "if norm:\n",
    "    y_train=nuf.normalize_data(y_train,r)\n",
    "    y_val=nuf.normalize_data(y_val,r)\n",
    "#np.savez(base_dir+\"check_r_distribution\",y_train=y_train,y_val=y_val) \n",
    "#rand_indexes=np.random.randint(0,len(y_train)-1,10000)\n",
    "#np.savez(base_dir+\"check_train_maps\",y_train=y_train[rand_indexes], x_train=x_train[rand_indexes])\n",
    "\n",
    "if map_norm:\n",
    "    for i in range(len(x_train)):\n",
    "        for j in range(n_inputs):\n",
    "            x=x_train[i,:,j]\n",
    "            x_train[i,:,j]=nuf.normalize_data(x,x)\n",
    "    for i in range(len(x_val)):\n",
    "        for j in range(n_inputs):\n",
    "            x=x_val[i,:,j]\n",
    "            x_val[i,:,j]=nuf.normalize_data(x,x)\n",
    "f_train=np.load(base_dir_tau+\"predictions.npz\")\n",
    "normalizer=f_train[\"norm\"]\n",
    "model_tau = keras.models.load_model(\n",
    "    base_dir_tau+test_model_folder,  custom_objects={'loss_training' : loss_training_tau, 'metrics' : metrics_tau}, \n",
    "    compile=False\n",
    ") #i restore the model from the test_model folder. I need to specify the custom objects and recompile the model with the custom\n",
    "#objects, thus the metrics and the loss functions\n",
    "model_tau.compile(loss=loss_training_tau,optimizer=tf.optimizers.Adam(), metrics=metrics_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    }
   ],
   "source": [
    "if n_output_tau==1:\n",
    "    predictions_train=model_tau.predict(x_train)\n",
    "    predictions_val=model_tau.predict(x_val)\n",
    "else:\n",
    "    predictions_train=model_tau.predict(x_train)[:,0].reshape((len(y_train), 1))\n",
    "    predictions_val=model_tau.predict(x_val)[:,0].reshape((len(y_val), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00021838711347700444\n",
      "(80, 1)\n",
      "(79,)\n"
     ]
    }
   ],
   "source": [
    "if norm:\n",
    "    y_train_sigma=(predictions_train-y_train)**2 * np.std(r)**2\n",
    "    y_train_nn=y_train_sigma[:]\n",
    "    print(np.mean(y_train_sigma))\n",
    "    print(y_train_sigma.shape)\n",
    "    y_val_sigma=(predictions_val-y_val)**2 * np.std(r)**2\n",
    "    count,red=uf.check_y(y_train_sigma)\n",
    "    print(red.shape)\n",
    "    y_train_sigma=nuf.normalize_data(y_train_sigma,red)\n",
    "    y_val_sigma=nuf.normalize_data(y_val_sigma,red)\n",
    "else:\n",
    "    y_train_sigma=(predictions_train-y_train)**2 \n",
    "    y_val_sigma=(predictions_val-y_val)**2 \n",
    "    red=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 1)\n",
      "[[2.02649880e-06]\n",
      " [8.21064237e-06]\n",
      " [1.98617610e-05]\n",
      " [2.70514202e-05]\n",
      " [4.39420570e-05]\n",
      " [3.88441817e-06]\n",
      " [1.15840949e-05]\n",
      " [1.25006668e-05]\n",
      " [1.48641418e-05]\n",
      " [2.34982664e-05]\n",
      " [2.22302669e-05]\n",
      " [3.49960746e-05]\n",
      " [3.36922121e-08]\n",
      " [7.02759913e-08]\n",
      " [1.32179088e-05]\n",
      " [2.67967144e-05]\n",
      " [3.13919450e-05]\n",
      " [9.30012563e-06]\n",
      " [1.76524276e-05]\n",
      " [2.28203370e-05]\n",
      " [3.50417152e-05]\n",
      " [5.76875237e-06]\n",
      " [1.30610381e-05]\n",
      " [9.79086292e-06]\n",
      " [8.52824287e-06]\n",
      " [4.50425154e-06]\n",
      " [2.69015567e-06]\n",
      " [7.07636049e-05]\n",
      " [8.83786867e-06]\n",
      " [1.01223726e-05]\n",
      " [1.70853223e-05]\n",
      " [2.17650212e-08]\n",
      " [2.70578689e-07]\n",
      " [2.82281634e-05]\n",
      " [1.48917054e-05]\n",
      " [3.61920244e-05]\n",
      " [2.64996748e-05]\n",
      " [2.80801919e-05]\n",
      " [1.39410626e-04]\n",
      " [8.62972282e-05]\n",
      " [1.09519698e-04]\n",
      " [1.07552422e-04]\n",
      " [2.67508747e-04]\n",
      " [2.77367931e-04]\n",
      " [3.67790311e-04]\n",
      " [6.29626091e-05]\n",
      " [5.55068746e-04]\n",
      " [3.82358665e-04]\n",
      " [2.15145119e-04]\n",
      " [2.89911320e-04]\n",
      " [2.96754029e-04]\n",
      " [1.61624181e-06]\n",
      " [1.37642037e-04]\n",
      " [2.74974310e-04]\n",
      " [3.94693021e-04]\n",
      " [1.33361071e-04]\n",
      " [4.96440723e-05]\n",
      " [7.41416835e-04]\n",
      " [6.01447715e-04]\n",
      " [8.16133065e-04]\n",
      " [4.79035328e-04]\n",
      " [5.47536783e-04]\n",
      " [8.77833604e-04]\n",
      " [2.64057198e-04]\n",
      " [4.18360913e-04]\n",
      " [9.05914647e-04]\n",
      " [3.60122991e-04]\n",
      " [3.06075863e-04]\n",
      " [3.29587754e-04]\n",
      " [6.76027665e-04]\n",
      " [5.25860635e-04]\n",
      " [6.71442164e-04]\n",
      " [6.30845902e-04]\n",
      " [1.53624351e-04]\n",
      " [9.73790926e-04]\n",
      " [6.22363667e-04]\n",
      " [7.35808823e-04]\n",
      " [4.40578092e-04]\n",
      " [2.51897841e-04]\n",
      " [3.25316087e-04]]\n"
     ]
    }
   ],
   "source": [
    "#print(predictions_train)\n",
    "print(y_train_sigma.shape)\n",
    "#print(red.shape)\n",
    "#print(red)\n",
    "#print(y_train_sigma.flatten())\n",
    "print(y_train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camb-kernel",
   "language": "python",
   "name": "camb-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
