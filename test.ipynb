{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 14:36:34.152258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-26 14:36:49.365179: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-26 14:36:49.383844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-26 14:36:49.383878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-06-26 14:37:05.862409: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-26 14:37:05.862561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (coka.fe.infn.it): /proc/driver/nvidia/version does not exist\n",
      "2023-06-26 14:37:05.943685: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "val_loss\n",
      "Epoch 1/400\n",
      "6/6 - 8s - loss: 1.7949 - val_loss: 30.4413 - 8s/epoch - 1s/step\n",
      "Epoch 2/400\n",
      "6/6 - 0s - loss: 1.2024 - val_loss: 1142.3387 - 355ms/epoch - 59ms/step\n",
      "Epoch 3/400\n",
      "6/6 - 0s - loss: 0.5383 - val_loss: 1335.3639 - 330ms/epoch - 55ms/step\n",
      "Epoch 4/400\n",
      "6/6 - 0s - loss: 0.5255 - val_loss: 3010.0596 - 331ms/epoch - 55ms/step\n",
      "Epoch 5/400\n",
      "6/6 - 0s - loss: 0.3653 - val_loss: 3335.2549 - 308ms/epoch - 51ms/step\n",
      "Epoch 6/400\n",
      "6/6 - 0s - loss: 0.4616 - val_loss: 1116.3910 - 334ms/epoch - 56ms/step\n",
      "Epoch 7/400\n",
      "6/6 - 0s - loss: 0.3627 - val_loss: 2148.7578 - 318ms/epoch - 53ms/step\n",
      "Epoch 8/400\n",
      "6/6 - 0s - loss: 0.2417 - val_loss: 1869.9094 - 316ms/epoch - 53ms/step\n",
      "Epoch 9/400\n",
      "6/6 - 0s - loss: 0.2074 - val_loss: 485.9981 - 319ms/epoch - 53ms/step\n",
      "Epoch 10/400\n",
      "6/6 - 0s - loss: 0.2333 - val_loss: 276.9058 - 311ms/epoch - 52ms/step\n",
      "Epoch 11/400\n",
      "6/6 - 0s - loss: 0.1852 - val_loss: 198.8037 - 310ms/epoch - 52ms/step\n",
      "Epoch 11: early stopping\n",
      "0 1142.3387451171875 11\n",
      "-------------------------------------------------\n",
      "10\n",
      "val_loss\n",
      "Epoch 1/400\n",
      "6/6 - 6s - loss: 1.4814 - val_loss: 2.4514 - 6s/epoch - 973ms/step\n",
      "Epoch 2/400\n",
      "6/6 - 0s - loss: 0.6269 - val_loss: 1071.2864 - 378ms/epoch - 63ms/step\n",
      "Epoch 3/400\n",
      "6/6 - 0s - loss: 0.3648 - val_loss: 1783.0408 - 344ms/epoch - 57ms/step\n",
      "Epoch 4/400\n",
      "6/6 - 0s - loss: 0.3249 - val_loss: 669.6331 - 345ms/epoch - 58ms/step\n",
      "Epoch 5/400\n",
      "6/6 - 0s - loss: 0.4067 - val_loss: 276.4107 - 340ms/epoch - 57ms/step\n",
      "Epoch 6/400\n",
      "6/6 - 0s - loss: 0.2059 - val_loss: 889.9765 - 362ms/epoch - 60ms/step\n",
      "Epoch 7/400\n",
      "6/6 - 0s - loss: 0.1579 - val_loss: 764.7601 - 352ms/epoch - 59ms/step\n",
      "Epoch 8/400\n",
      "6/6 - 0s - loss: 0.2874 - val_loss: 136.7802 - 346ms/epoch - 58ms/step\n",
      "Epoch 9/400\n",
      "6/6 - 0s - loss: 0.2203 - val_loss: 125.2961 - 328ms/epoch - 55ms/step\n",
      "Epoch 10/400\n",
      "6/6 - 0s - loss: 0.1725 - val_loss: 99.3740 - 349ms/epoch - 58ms/step\n",
      "Epoch 11/400\n",
      "6/6 - 0s - loss: 0.2099 - val_loss: 24.8236 - 319ms/epoch - 53ms/step\n",
      "Epoch 11: early stopping\n",
      "1 1071.286376953125 11\n",
      "-------------------------------------------------\n",
      "10\n",
      "val_loss\n",
      "Epoch 1/400\n",
      "6/6 - 6s - loss: 1.6241 - val_loss: 4.7982 - 6s/epoch - 989ms/step\n",
      "Epoch 2/400\n",
      "6/6 - 0s - loss: 1.1353 - val_loss: 1728.5364 - 350ms/epoch - 58ms/step\n",
      "Epoch 3/400\n",
      "6/6 - 0s - loss: 0.6848 - val_loss: 1694.0181 - 335ms/epoch - 56ms/step\n",
      "Epoch 4/400\n",
      "6/6 - 0s - loss: 0.5657 - val_loss: 4607.0039 - 329ms/epoch - 55ms/step\n",
      "Epoch 5/400\n",
      "6/6 - 0s - loss: 0.3814 - val_loss: 4790.3330 - 324ms/epoch - 54ms/step\n",
      "Epoch 6/400\n",
      "6/6 - 0s - loss: 0.2830 - val_loss: 5931.0688 - 320ms/epoch - 53ms/step\n",
      "Epoch 7/400\n",
      "6/6 - 0s - loss: 0.5073 - val_loss: 3402.4282 - 333ms/epoch - 55ms/step\n",
      "Epoch 8/400\n",
      "6/6 - 0s - loss: 0.3419 - val_loss: 1726.0863 - 334ms/epoch - 56ms/step\n",
      "Epoch 9/400\n",
      "6/6 - 0s - loss: 0.3574 - val_loss: 773.9881 - 342ms/epoch - 57ms/step\n",
      "Epoch 10/400\n",
      "6/6 - 0s - loss: 0.2831 - val_loss: 787.5564 - 336ms/epoch - 56ms/step\n",
      "Epoch 11/400\n",
      "6/6 - 0s - loss: 0.2462 - val_loss: 508.3350 - 325ms/epoch - 54ms/step\n",
      "Epoch 11: early stopping\n",
      "2 1728.536376953125 11\n",
      "-------------------------------------------------\n",
      "10\n",
      "val_loss\n",
      "Epoch 1/400\n",
      "6/6 - 6s - loss: 1.6257 - val_loss: 1.3047 - 6s/epoch - 961ms/step\n",
      "Epoch 2/400\n",
      "6/6 - 0s - loss: 0.9315 - val_loss: 644.2691 - 328ms/epoch - 55ms/step\n",
      "Epoch 3/400\n",
      "6/6 - 0s - loss: 0.6127 - val_loss: 1369.8594 - 322ms/epoch - 54ms/step\n",
      "Epoch 4/400\n",
      "6/6 - 0s - loss: 0.4784 - val_loss: 247.8359 - 331ms/epoch - 55ms/step\n",
      "Epoch 5/400\n",
      "6/6 - 0s - loss: 0.2720 - val_loss: 755.2694 - 323ms/epoch - 54ms/step\n",
      "Epoch 6/400\n",
      "6/6 - 0s - loss: 0.1780 - val_loss: 114.0339 - 320ms/epoch - 53ms/step\n",
      "Epoch 7/400\n",
      "6/6 - 0s - loss: 0.2067 - val_loss: 203.7935 - 316ms/epoch - 53ms/step\n",
      "Epoch 8/400\n",
      "6/6 - 0s - loss: 0.2372 - val_loss: 1033.7692 - 324ms/epoch - 54ms/step\n",
      "Epoch 9/400\n",
      "6/6 - 0s - loss: 0.1403 - val_loss: 464.0111 - 326ms/epoch - 54ms/step\n",
      "Epoch 10/400\n",
      "6/6 - 0s - loss: 0.2378 - val_loss: 93.2060 - 327ms/epoch - 54ms/step\n",
      "Epoch 11/400\n",
      "6/6 - 0s - loss: 0.1419 - val_loss: 141.4489 - 331ms/epoch - 55ms/step\n",
      "Epoch 11: early stopping\n",
      "3 644.2691040039062 11\n",
      "-------------------------------------------------\n",
      "10\n",
      "val_loss\n",
      "Epoch 1/400\n",
      "6/6 - 6s - loss: 2.1200 - val_loss: 3.1842 - 6s/epoch - 951ms/step\n",
      "Epoch 2/400\n",
      "6/6 - 0s - loss: 1.1867 - val_loss: 92.6081 - 328ms/epoch - 55ms/step\n",
      "Epoch 3/400\n",
      "6/6 - 0s - loss: 0.9785 - val_loss: 72.5795 - 332ms/epoch - 55ms/step\n",
      "Epoch 4/400\n",
      "6/6 - 0s - loss: 0.7909 - val_loss: 106.0928 - 304ms/epoch - 51ms/step\n",
      "Epoch 5/400\n",
      "6/6 - 0s - loss: 0.6127 - val_loss: 210.5108 - 312ms/epoch - 52ms/step\n",
      "Epoch 6/400\n",
      "6/6 - 0s - loss: 0.6350 - val_loss: 290.2038 - 310ms/epoch - 52ms/step\n",
      "Epoch 7/400\n",
      "6/6 - 0s - loss: 0.4719 - val_loss: 276.0456 - 321ms/epoch - 54ms/step\n",
      "Epoch 8/400\n",
      "6/6 - 0s - loss: 0.5254 - val_loss: 190.9213 - 327ms/epoch - 55ms/step\n",
      "Epoch 9/400\n",
      "6/6 - 0s - loss: 0.4715 - val_loss: 145.5191 - 320ms/epoch - 53ms/step\n",
      "Epoch 10/400\n",
      "6/6 - 0s - loss: 0.4015 - val_loss: 82.7758 - 321ms/epoch - 53ms/step\n",
      "Epoch 11/400\n",
      "6/6 - 0s - loss: 0.4205 - val_loss: 35.4138 - 322ms/epoch - 54ms/step\n",
      "Epoch 11: early stopping\n",
      "4 92.60814666748047 11\n",
      "-------------------------------------------------\n",
      "10\n",
      "val_loss\n",
      "Epoch 1/400\n",
      "6/6 - 5s - loss: 1.3680 - val_loss: 75.9527 - 5s/epoch - 890ms/step\n",
      "Epoch 2/400\n",
      "6/6 - 0s - loss: 0.8960 - val_loss: 33.0457 - 320ms/epoch - 53ms/step\n",
      "Epoch 3/400\n",
      "6/6 - 0s - loss: 0.8030 - val_loss: 0.4597 - 326ms/epoch - 54ms/step\n",
      "Epoch 4/400\n",
      "6/6 - 0s - loss: 0.6087 - val_loss: 381.2831 - 322ms/epoch - 54ms/step\n",
      "Epoch 5/400\n",
      "6/6 - 0s - loss: 0.3454 - val_loss: 722.3163 - 324ms/epoch - 54ms/step\n",
      "Epoch 6/400\n",
      "6/6 - 0s - loss: 0.3123 - val_loss: 503.8011 - 328ms/epoch - 55ms/step\n",
      "Epoch 7/400\n",
      "6/6 - 0s - loss: 0.2928 - val_loss: 416.8619 - 325ms/epoch - 54ms/step\n",
      "Epoch 8/400\n",
      "6/6 - 0s - loss: 0.3721 - val_loss: 216.1333 - 325ms/epoch - 54ms/step\n",
      "Epoch 9/400\n",
      "6/6 - 0s - loss: 0.2890 - val_loss: 125.6228 - 320ms/epoch - 53ms/step\n",
      "Epoch 10/400\n",
      "6/6 - 0s - loss: 0.2797 - val_loss: 89.3057 - 332ms/epoch - 55ms/step\n",
      "Epoch 11/400\n",
      "6/6 - 0s - loss: 0.1848 - val_loss: 57.9934 - 327ms/epoch - 54ms/step\n",
      "Epoch 12/400\n",
      "6/6 - 0s - loss: 0.2082 - val_loss: 35.3072 - 325ms/epoch - 54ms/step\n",
      "Epoch 13/400\n",
      "6/6 - 0s - loss: 0.2416 - val_loss: 21.2861 - 318ms/epoch - 53ms/step\n",
      "Epoch 13: early stopping\n",
      "5 381.2830810546875 13\n",
      "-------------------------------------------------\n",
      "10\n",
      "val_loss\n",
      "Epoch 1/400\n",
      "6/6 - 6s - loss: 1.9437 - val_loss: 263.8753 - 6s/epoch - 1s/step\n",
      "Epoch 2/400\n",
      "6/6 - 0s - loss: 1.2453 - val_loss: 883.6923 - 319ms/epoch - 53ms/step\n",
      "Epoch 3/400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 156\u001b[0m\n\u001b[1;32m    150\u001b[0m y_v\u001b[38;5;241m=\u001b[39my_val[train_distr][index_val]\n\u001b[1;32m    152\u001b[0m model\u001b[38;5;241m=\u001b[39mnuf\u001b[38;5;241m.\u001b[39mbuild_network(n_inputs,nside,n_layers\u001b[38;5;241m=\u001b[39mn_layers,layer_nodes\u001b[38;5;241m=\u001b[39mnodes_per_layer,\n\u001b[1;32m    153\u001b[0m                     num_output\u001b[38;5;241m=\u001b[39mn_output,use_normalization\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mFalse\u001b[39;00m,use_normalization,use_normalization],\n\u001b[1;32m    154\u001b[0m                     use_drop\u001b[38;5;241m=\u001b[39muse_drop,drop\u001b[38;5;241m=\u001b[39m[drop,drop,drop],\n\u001b[1;32m    155\u001b[0m                     activation_dense\u001b[38;5;241m=\u001b[39mactivation,kernel_initializer\u001b[38;5;241m=\u001b[39mkernel)\n\u001b[0;32m--> 156\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mnuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mstopping_monitor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreduce_monitor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf_reduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_reduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mloss_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m history_d\u001b[38;5;241m=\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory\n\u001b[1;32m    160\u001b[0m val_loss\u001b[38;5;241m=\u001b[39mhistory_d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m]\n",
      "File \u001b[0;32m~/pipeline/NN_functions.py:144\u001b[0m, in \u001b[0;36mcompile_and_fit\u001b[0;34m(model, x_train, y_train, x_val, y_val, batch_size, max_epochs, stopping_monitor, p_stopping, reduce_monitor, f_reduce, p_reduce, base_dir, loss_training, lr, metrics, shuffle, verbose, callbacks, append, n_optimizer)\u001b[0m\n\u001b[1;32m    140\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss_training,\n\u001b[1;32m    141\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39moptimizers[n_optimizer],\n\u001b[1;32m    142\u001b[0m               metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m#i define the loss function, optimizer and metrics that i am using in the training\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/.conda/envs/camb/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/camb/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/camb/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/camb/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/camb/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/camb/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/camb/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/camb/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/camb/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import nnhealpix.layers\n",
    "from tensorflow.keras import metrics\n",
    "import pandas as pd\n",
    "from loss_functions import sigma_loss, sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma, mse_batch, sigma_f_loss\n",
    "import math\n",
    "import useful_functions as uf\n",
    "import NN_functions as nuf\n",
    "import os, shutil\n",
    "import keras_tuner\n",
    "\n",
    "seed_train=40\n",
    "np.random.seed(seed_train)# i set a random seed for the generation of the maps for reproducibility\n",
    "tf.random.set_seed(seed_train)#the seed for tensorflow operation is different from the seed for numpy operations\n",
    "\n",
    "nside = 16\n",
    "n_train=1000\n",
    "n_train_fix=400 #the total number of of training maps i will spread on all the r interval -> for each r value i generate n_train_fix/len(r) maps \n",
    "kind_of_map=\"BB\"\n",
    "n_channels=2\n",
    "pol=1\n",
    "res=hp.nside2resol(nside, arcmin=False) \n",
    "sensitivity=4\n",
    "\n",
    "#train and val\n",
    "fval=[0.1,0.2] # this is the fraction of data that i use for validation, computed on n_train_fix\n",
    "norm=True\n",
    "map_norm=True\n",
    "batch_ordering=False\n",
    "batch_size=16\n",
    "n_inputs=2\n",
    "max_epochs=400\n",
    "\n",
    "n_layers=2\n",
    "nodes_per_layer=[256,128]\n",
    "n_output=1\n",
    "use_drop=[False,True,True]\n",
    "\n",
    "stopping_monitor=\"val_loss\"\n",
    "reduce_monitor=\"val_loss\"\n",
    "p_stopping=10\n",
    "f_reduce=1\n",
    "p_reduce=5\n",
    "callbacks=[True,False,False,False,False]\n",
    "base_dir=\"/home/amorelli/r_estimate/B_maps_white_noise/tuning/26_6_23\"\n",
    "loss_training=tf.keras.losses.MeanSquaredError()\n",
    "metrics=[]\n",
    "\n",
    "f_ = np.load('/home/amorelli/cl_generator/outfile_R_000_001_seed=67.npz') \n",
    "#print(\"outfile_R:\",f_.files) #give the keiwords for the stored arrays\n",
    "labels=f_.files\n",
    "data=f_[labels[0]]\n",
    "r=f_[labels[1]]\n",
    "r, data=uf.unison_sorted_copies(r, data)\n",
    "indexes=np.linspace(0,len(r)-1,10,dtype=int)\n",
    "r=r[indexes]\n",
    "data=data[indexes]\n",
    "\n",
    "#input_folder=\"/home/amorelli/foreground_noise_maps/noise_generation\"\n",
    "#input_files=os.listdir(input_folder)\n",
    "#for i in range(len(input_files)):\n",
    "   # input_files[i]=input_folder+\"/\"+input_files[i]\n",
    "noise_maps=uf.generate_noise_maps(n_train,n_channels,nside,pol=1,sensitivity=sensitivity,input_files=None)\n",
    "\n",
    "#noise_E,noise_B=uf.convert_to_EB(noise_maps)\n",
    "maps_per_cl=[]\n",
    "mappe_B=[]\n",
    "y_r=[]\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "x_val=[]\n",
    "y_val=[]\n",
    "for i in range(2):\n",
    "    maps_per_cl_gen=uf.maps_per_cl(distribution=i)\n",
    "    maps_per_cl.append(maps_per_cl_gen.compute_maps_per_cl(r,n_train,n_train_fix))\n",
    "    mappe,y_mappe=uf.generate_maps(data, r,n_train=n_train,nside=nside, map_per_cl=maps_per_cl[i], \n",
    "                             noise_maps=noise_maps, beam_w=2*res, kind_of_map=kind_of_map, \n",
    "                             raw=0 , n_channels=n_channels,beam_yes=1 , verbose=0)\n",
    "    mappe_B.append(mappe)\n",
    "    y_r.append(y_mappe)\n",
    "    x_t,y_t,x_v,y_v = nuf.prepare_data(y_r[i],mappe_B[i],r,n_train,n_train_fix,fval[i],maps_per_cl[i]\n",
    "                                               , batch_size, batch_ordering=batch_ordering)\n",
    "    x_train.append(x_t)\n",
    "    y_train.append(y_t)\n",
    "    x_val.append(x_v)\n",
    "    y_val.append(y_v)\n",
    "    \n",
    "    if norm:\n",
    "        y_train[i]=nuf.normalize_data(y_train[i],r)\n",
    "        y_val[i]=nuf.normalize_data(y_val[i],r)\n",
    "    if map_norm:\n",
    "        for k in range(len(x_train[i])):\n",
    "            for j in range(n_inputs):\n",
    "                x=x_train[i][k,:,j]\n",
    "                x_train[i][k,:,j]=nuf.normalize_data(x,x)\n",
    "        for k in range(len(x_val[i])):\n",
    "            for j in range(n_inputs):\n",
    "                x=x_val[i][k,:,j]\n",
    "                x_val[i][k,:,j]=nuf.normalize_data(x,x)\n",
    "parameters={\n",
    "    \"lr\":[0.01 * (0.1)**i for i in range(5)],\n",
    "    \"kernel\":[\"he_normal\",\"glorot_uniform\"],\n",
    "    \"optimizers\":[0,1],\n",
    "    \"use_normalization\":[True,False],\n",
    "    \"drop\":[0.2,0.5],\n",
    "    \"distr\":[0,1],\n",
    "    \"activation\":[\"relu\",\"swish\"]\n",
    "}\n",
    "n_param=len(parameters)\n",
    "all_configurations=[]\n",
    "def select_indices(n_param, parameters, configuration, depth):\n",
    "    key=list(parameters.keys())[depth]\n",
    "    dim=len(parameters[key])\n",
    "    if depth==n_param-1:\n",
    "        for i in range(dim):\n",
    "            configuration[depth]=i\n",
    "            all_configurations.append(list(configuration))\n",
    "        #print(all_configurations)\n",
    "        return None\n",
    "    else:\n",
    "        for i in range(dim):\n",
    "            configuration[depth]=i\n",
    "            select_indices(n_param, parameters, configuration, depth+1)\n",
    "select_indices(n_param, parameters, [0 for i in range(n_param)],0)\n",
    "st=0\n",
    "for index,list_index in enumerate(all_configurations[st:10]):\n",
    "    lr=parameters[\"lr\"][list_index[0]]\n",
    "    kernel=parameters[\"kernel\"][list_index[1]]\n",
    "    n_optimizer=parameters[\"optimizers\"][list_index[2]]\n",
    "    use_normalization=parameters[\"use_normalization\"][list_index[3]]\n",
    "    drop=parameters[\"drop\"][list_index[4]]\n",
    "    train_distr=parameters[\"distr\"][list_index[5]]\n",
    "    activation=parameters[\"activation\"][list_index[6]]\n",
    "    \n",
    "    n_train=len(y_train[train_distr])\n",
    "    n_val=len(y_val[train_distr])\n",
    "    f_tune=0.1\n",
    "\n",
    "    index_train=np.random.randint(0,n_train,math.ceil(n_train*f_tune))\n",
    "    index_val=np.random.randint(0,n_val,math.ceil(n_val*f_tune))\n",
    "\n",
    "    x_t=x_train[train_distr][index_train]\n",
    "    x_v=x_val[train_distr][index_val]\n",
    "    y_t=y_train[train_distr][index_train]\n",
    "    y_v=y_val[train_distr][index_val]\n",
    "    \n",
    "    model=nuf.build_network(n_inputs,nside,n_layers=n_layers,layer_nodes=nodes_per_layer,\n",
    "                        num_output=n_output,use_normalization=[False,use_normalization,use_normalization],\n",
    "                        use_drop=use_drop,drop=[drop,drop,drop],\n",
    "                        activation_dense=activation,kernel_initializer=kernel)\n",
    "    history=nuf.compile_and_fit(model, x_t, y_t, x_v, y_v, batch_size, max_epochs, \n",
    "                                stopping_monitor,p_stopping,reduce_monitor,f_reduce, p_reduce,base_dir, \n",
    "                                loss_training,lr,metrics,shuffle=True, verbose=2,callbacks=callbacks,n_optimizer=n_optimizer)\n",
    "    history_d=history.history\n",
    "    val_loss=history_d[\"val_loss\"][-10]\n",
    "    last_epoch=len(history_d[\"val_loss\"])\n",
    "    lista=[str(index+st),str(val_loss),str(last_epoch)]\n",
    "    print(index+st,val_loss,last_epoch)\n",
    "    print(\"-------------------------------------------------\")\n",
    "    with open(base_dir+\"/\"+'history.txt',\"a\") as f:\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\" \".join(lista))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camb-kernel",
   "language": "python",
   "name": "camb-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
