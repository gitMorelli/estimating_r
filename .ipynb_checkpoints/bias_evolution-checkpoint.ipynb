{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 14:55:19.082840: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import nnhealpix.layers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import metrics\n",
    "import keras\n",
    "import pandas as pd\n",
    "import os, shutil\n",
    "import useful_functions as uf\n",
    "import NN_functions as nuf\n",
    "from loss_functions import sigma_loss, sigma_f_loss, sigma2_loss,sigma_batch_loss,sigma_norm_loss,sigma_log_loss,mse_tau,mse_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir=\"/home/amorelli/r_estimate/B_maps_white_noise/results_18_6_overfitting/\"\n",
    "load_dir='/home/amorelli/r_estimate/B_maps_white_noise/'\n",
    "to_load=\"test_data_r_0_001_t006_40.npz\"\n",
    "\n",
    "#home_dir=\"/home/amorelli/E_foreground/17_6_23/\"\n",
    "#load_dir='/home/amorelli/E_foreground/' \n",
    "#to_load=\"test_data_r000_t001_013_411.npz\"\n",
    "\n",
    "loss_training=tf.keras.losses.MeanSquaredError()\n",
    "#loss_training=sigma_batch_loss\n",
    "\n",
    "checkpoint_dir=home_dir+\"checkpoints\"\n",
    "last=15\n",
    "normalize=True\n",
    "test_model_folder=\"test_model\"\n",
    "n_output=1\n",
    "metrics=[]\n",
    "map_norm=False\n",
    "nodes_per_layer=[256,128,128]\n",
    "nside=16\n",
    "drop=0.0\n",
    "n_layers=3\n",
    "n_inputs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=np.load(load_dir+to_load)\n",
    "x_test=f[\"x_test\"]\n",
    "y_test=f[\"y_test\"]\n",
    "y_test,x_test=uf.unison_sorted_copies(y_test,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train=np.load(home_dir+\"predictions.npz\")\n",
    "y_train=f_train[\"y_train\"]\n",
    "predictions_train=f_train[\"pred\"]\n",
    "norm=f_train[\"norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nuf.build_network(n_inputs,nside,n_layers=n_layers,layer_nodes=nodes_per_layer,\n",
    "                        num_output=n_output,use_normalization=[False,False,False],\n",
    "                        use_drop=[False,True,False],drop=[drop,drop,drop],\n",
    "                        activation_dense=\"relu\",kernel_initializer=\"glorot_uniform\")\n",
    "model.compile(loss=loss_training, optimizer=tf.optimizers.Adam(),metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=os.listdir(checkpoint_dir)\n",
    "for f in files:\n",
    "    splitted = f.split(\"-\")\n",
    "    if int(splitted[2])==last:\n",
    "        checkpoint_path=f\n",
    "        break\n",
    "print(checkpoint_path)\n",
    "model.load_weights(checkpoint_dir+'/'+ checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = keras.models.load_model(\n",
    "    home_dir+test_model_folder,  custom_objects={'loss_training' : loss_training, 'metrics' : metrics}, \n",
    "    compile=False\n",
    ") #i restore the model from the test_model folder. I need to specify the custom objects and recompile the model with the custom\n",
    "#objects, thus the metrics and the loss functions\n",
    "model.compile(loss=loss_training,optimizer=tf.optimizers.Adam(), metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#results=model.evaluate(x,y,verbose=2)\n",
    "if map_norm:\n",
    "    for i in range(len(x_test)):\n",
    "        for j in range(n_inputs):\n",
    "            t=x_test[i,:,j]\n",
    "            x_test[i,:,j]=nuf.normalize_data(t,t)\n",
    "predictions=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_output>1:\n",
    "    predictions=predictions[:,0]\n",
    "    y_pred=predictions_train[:,0]\n",
    "else:\n",
    "    y_pred=predictions_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize:\n",
    "    y_pred=nuf.denormalize_data(y_pred,norm)\n",
    "    y_train=nuf.denormalize_data(y_train,norm)\n",
    "    predictions=nuf.denormalize_data(predictions,norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions=-predictions+0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tau_pred=predictions[:,0]\n",
    "sigma_pred=np.abs(predictions[:,1])#*predictions[:,1]\n",
    "tau_est=np.mean(tau_pred)\n",
    "sigma_tau=np.std(tau_pred)**2\n",
    "#sigma_tau=np.sum((tau_pred-0.06)**2)/10000\n",
    "sigma_est=np.mean(sigma_pred)\n",
    "counts_tau, bins_tau = np.histogram(tau_pred)\n",
    "counts_sigma, bins_sigma = np.histogram(sigma_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_count, y_bias = uf.check_y(y_test)\n",
    "b = predictions.flatten()-y_test.flatten()\n",
    "bias = uf.running_average(b,y_count,y_bias)\n",
    "pred = uf.running_average(predictions,y_count,y_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "#print(y_bias)\n",
    "plt.plot(y_bias,bias)\n",
    "plt.plot(y_bias,pred)\n",
    "plt.plot(y_bias,y_bias)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(y_bias,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_pred = uf.unison_sorted_copies(y_train,y_pred)\n",
    "y_count_train, y_bias_train = uf.check_y(y_train.flatten())\n",
    "b_train = y_pred.flatten()-y_train.flatten()\n",
    "bias_train = uf.running_average(b_train,y_count_train,y_bias_train)\n",
    "pred_train = uf.running_average(y_pred,y_count_train,y_bias_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(y_bias_train,bias_train)\n",
    "plt.plot(y_bias_train,pred_train)\n",
    "plt.plot(y_bias_train,y_bias_train)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(y_bias_train,bias_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camb-kernel",
   "language": "python",
   "name": "camb-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
